{"cells":[{"cell_type":"markdown","metadata":{"id":"rhBFqimPrz_G"},"source":["# Persian Stance Classification - Deep Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YIVCRuwZrz_H"},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import numpy as np\n","import numpy as np\n","import os.path as path"]},{"cell_type":"markdown","metadata":{"id":"SfpzAw0Orz_H"},"source":["# Mount Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693062808869,"user_tz":-210,"elapsed":33453,"user":{"displayName":"Vahid Kiani","userId":"06632536328202809770"}},"outputId":"2d5823d5-b0c9-4833-f27a-537e7d628cc3","id":"9IEI65wtrz_I"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGTCPoknrz_J"},"outputs":[],"source":["# input files\n","cleaned_path = \"/content/drive/MyDrive/Stance Detection Project/dataset cleaned/Clean_Claim_Body.csv\"\n","train_path = \"/content/drive/MyDrive/Stance Detection Project/dataset cleaned/train_data.csv\"\n","test_path = \"/content/drive/MyDrive/Stance Detection Project/dataset cleaned/test_data.csv\"\n","\n","fasttext_path = \"/content/drive/MyDrive/Stance Detection Project/Baseline Zarharan/cc.fa.300.vec\"   # this is a text file containing FastText word embeddings for Farsi\n","\n","# output file\n","sequences_file = '/content/drive/MyDrive/Stance Detection Project/Baseline Zarharan/features/sequences.npy'\n","FEATURES_DIR = '/content/drive/MyDrive/Stance Detection Project/Baseline Zarharan/features/'"]},{"cell_type":"markdown","metadata":{"id":"9m4-v6yxrz_K"},"source":["# Read Cleaned Data from CSV File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IeqjNe6Trz_K"},"outputs":[],"source":["import pandas as pd\n","dataset_clean = pd.read_csv(cleaned_path, index_col = 0, )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsLA8whUrz_L"},"outputs":[],"source":["clean_claim = dataset_clean['claim']\n","clean_body = dataset_clean['body']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1693062809784,"user_tz":-210,"elapsed":60,"user":{"displayName":"Vahid Kiani","userId":"06632536328202809770"}},"outputId":"ed8a4fb7-e685-4ec5-a9a0-2194bce7f4cd","id":"Y1Cc9xwTrz_M"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               claim  \\\n","0         کلاهبرداری از رانندگان با شگرد نشت بنزین !   \n","1  تجاوز به دختر بازداشت شده و واژگونی ون گشت ارش...   \n","2  تعظیم 20 دقیقه ای وزیر نیرو ژاپن به علت قطع بر...   \n","3              سرمربیگری گاس هیدینک برای تراکتورسازی   \n","4  کشف موجود عجیبی شبیه انسان در یک حفاری در پاکس...   \n","\n","                                                body      label  \n","0  به گزارش خبرنگار گروه جامعه خبرگزاری میزان،29 ...    Discuss  \n","1  انتشار کلیپ واژگونی ماشین گشت ارشاد توسط مردم ...    Discuss  \n","2  وزیر نیروی ژاپن به علت قطع شدن برق؛ به همان مد...      Agree  \n","3  به تازگی محمد تقوی استعفای خود را از سرمربیگری...    Discuss  \n","4  پس از 20 سال حفاری با دقتی باورنکردنی، سرانجام...  Unrelated  "],"text/html":["\n","  <div id=\"df-872d5888-8411-450e-bdd1-5d7786f7a61a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>claim</th>\n","      <th>body</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>کلاهبرداری از رانندگان با شگرد نشت بنزین !</td>\n","      <td>به گزارش خبرنگار گروه جامعه خبرگزاری میزان،29 ...</td>\n","      <td>Discuss</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>تجاوز به دختر بازداشت شده و واژگونی ون گشت ارش...</td>\n","      <td>انتشار کلیپ واژگونی ماشین گشت ارشاد توسط مردم ...</td>\n","      <td>Discuss</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>تعظیم 20 دقیقه ای وزیر نیرو ژاپن به علت قطع بر...</td>\n","      <td>وزیر نیروی ژاپن به علت قطع شدن برق؛ به همان مد...</td>\n","      <td>Agree</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>سرمربیگری گاس هیدینک برای تراکتورسازی</td>\n","      <td>به تازگی محمد تقوی استعفای خود را از سرمربیگری...</td>\n","      <td>Discuss</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>کشف موجود عجیبی شبیه انسان در یک حفاری در پاکس...</td>\n","      <td>پس از 20 سال حفاری با دقتی باورنکردنی، سرانجام...</td>\n","      <td>Unrelated</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-872d5888-8411-450e-bdd1-5d7786f7a61a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-872d5888-8411-450e-bdd1-5d7786f7a61a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-872d5888-8411-450e-bdd1-5d7786f7a61a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-49271e04-04a7-4f44-baa1-804c95025adc\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-49271e04-04a7-4f44-baa1-804c95025adc')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const charts = await google.colab.kernel.invokeFunction(\n","          'suggestCharts', [key], {});\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-49271e04-04a7-4f44-baa1-804c95025adc button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":6}],"source":["dataset_clean.head()"]},{"cell_type":"markdown","metadata":{"id":"GKZSluimrz_N"},"source":["# Read Data Train and Test from CSV Files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDOZ1C5Prz_N"},"outputs":[],"source":["data_train = pd.read_csv(train_path, index_col = 0, )\n","data_test = pd.read_csv(test_path, index_col = 0, )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693062810880,"user_tz":-210,"elapsed":15,"user":{"displayName":"Vahid Kiani","userId":"06632536328202809770"}},"outputId":"ff592d40-04a0-4c63-872d-749d47fa574f","id":"TAmnGhzIrz_O"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1597, 400)"]},"metadata":{},"execution_count":8}],"source":["len(data_train), len(data_test)"]},{"cell_type":"markdown","metadata":{"id":"k6BwwgST7Mzw"},"source":["# Download Fasttext Word Vectors File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBQdiEfv4WLO"},"outputs":[],"source":["# download fasttext word vectors for persian (text file version)\n","# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\n","# ------------------------------------------------------------------------------\n","# unzip fasttext word vectors for persian\n","# !gunzip cc.fa.300.vec.gz\n","\n","# then copy unziped text file to you google drive for future use"]},{"cell_type":"markdown","metadata":{"id":"r47z4bBZ7Tso"},"source":["# Load Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BQ1bFu7UBdk"},"outputs":[],"source":["def load_embedding_pandas(FILE, type=\"w2v\"):\n","  embeddings_index=dict()\n","  f = open(FILE)\n","  for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","  f.close()\n","  print('Loaded %s word vectors.' % len(embeddings_index))\n","  return embeddings_index"]},{"cell_type":"markdown","metadata":{"id":"z2dMrV2_90Jo"},"source":["# Reduction Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5skj76c3apYu"},"outputs":[],"source":["import pickle\n","import nltk\n","\n","def create_embedding_lookup_pandas(text_list, max_nb_words, embedding_dim, embedding,\n","                            embedding_lookup_name, embedding_vocab_name, rdm_emb_init=False, add_unknown=False, tokenizer=None, init_zeros = False):\n","    \"\"\"\n","    Creates the claim embedding lookup table if it not already exists and returns the vocabulary for it\n","    :param text_list:\n","    :param max_nb_words:\n","    :param embedding_dim:\n","    :param GloVe_vectors:\n","    :param embedding_lookup_name:\n","    :param embedding_vocab_name:\n","    :return:\n","    \"\"\"\n","    #del GloVe_vectors\n","\n","    # if ...embedding.npy or vocab.pkl files don't exist:\n","    if not path.exists(FEATURES_DIR + embedding_lookup_name) or not path.exists(FEATURES_DIR + embedding_vocab_name):\n","        print(\"can't find npy or pkl file!\")\n","        vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=None, tokenizer=tokenizer,\n","                                            max_features=max_nb_words, use_idf=True)\n","        vectorizer.fit_transform(text_list)\n","        vocab = vectorizer.vocabulary_\n","\n","\n","        # do not use 0 since we want to use masking in the LSTM later on\n","        for word in vocab.keys():\n","            vocab[word] += 1\n","        if add_unknown == True:\n","            max_index = max(vocab.values())\n","            vocab[\"UNKNOWN\"] = max_index+1\n","\n","        # prepare embedding - create matrix that holds the glove vector for each vocab entry\n","        if rdm_emb_init == True:\n","            embedding_lookup = np.random.random((len(vocab) + 1, embedding_dim))\n","            zero_vec = np.zeros((embedding_dim))\n","            embedding_lookup[0] = zero_vec # for masking\n","        else:\n","            embedding_lookup = np.zeros((len(vocab) + 1, embedding_dim))\n","\n","        if init_zeros == False:\n","            for word, i in vocab.items():\n","                if word == \"UNKNOWN\":\n","                    embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n","                    #print(embedding_vector)\n","                else:\n","                    try:\n","                        embedding_vector = embedding.loc[word].as_matrix()\n","                    except KeyError: #https://stackoverflow.com/questions/15653966/ignore-keyerror-and-continue-program\n","                        continue\n","                if embedding_vector is not None:\n","                    # words not found in embedding index will be all-zeros.\n","                    embedding_lookup[i] = embedding_vector\n","        print(\"created embedding lookup!\")\n","        #print(embedding_lookup[-1])\n","        # save embedding matrix\n","        np.save(FEATURES_DIR + embedding_lookup_name, embedding_lookup)\n","        print(\"embedding matrix saved!\")\n","        # save vocab\n","        with open(FEATURES_DIR + embedding_vocab_name, 'wb') as f:\n","            pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n","        print(\"vocab saved!\")\n","\n","        print(\"Embedding lookup table shape for \" + embedding_lookup_name + \" is: \" + str(embedding_lookup.shape))\n","    #if both .npy and .pkl files exist:\n","    else:\n","        print(\"found npy and pkl files!\")\n","        with open(FEATURES_DIR + embedding_vocab_name, \"rb\") as f:\n","            vocab = pickle.load(f)\n","\n","    print(\"Vocab size for \" + embedding_vocab_name + \" is: \" + str(len(vocab)))\n","\n","    return vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9uMrbpUYE9m"},"outputs":[],"source":["def text_to_sequences_fixed_size(texts, vocab, MAX_SENT_LENGTH, save_full_text=False, take_full_claim = False):\n","    \"\"\"\n","    Turns sentences of claims into sequences of indices provided by the given vocab.\n","    Unknown words will get an extra index, if\n","    the vocab has a token \"UNKNOWN\". The method takes the longest sentence of the claims, if the\n","    claim should have more than one sentence.\n","    :param texts:\n","    :param vocab:\n","    :param MAX_SENT_LENGTH:\n","    :return:\n","    \"\"\"\n","    data = np.zeros((len(texts), MAX_SENT_LENGTH), dtype='int32')\n","\n","    claims = []\n","    if take_full_claim == False:\n","        for claim in texts:\n","            claim_sents = nltk.sent_tokenize(claim)\n","            word_count_fct = lambda sentence: len(nltk.word_tokenize(sentence)) # take longest sentence of claim if it has more than one\n","            claims.append(max(claim_sents, key=word_count_fct))\n","    else:\n","        claims = texts\n","\n","    data_string_dict = {}\n","    for j, claim in tqdm(enumerate(claims)):\n","        claim_tokens = nltk.word_tokenize(claim.lower())\n","\n","        data_string = \"\"\n","        if save_full_text == True:\n","            for token in claim_tokens:\n","                data_string += token + \" \"\n","            data_string = data_string[:-1]\n","            data_string_dict[j] = data_string\n","\n","        for i, token in enumerate(claim_tokens):\n","            if i < MAX_SENT_LENGTH:\n","                index = vocab.get(token, \"UNKNOWN\")\n","                if index == \"UNKNOWN\":\n","                    index = vocab.get(index, None)\n","                if index != None:\n","                    data[j, i] = index\n","\n","    if save_full_text == True:\n","        return data, data_string_dict\n","    else:\n","        return data\n","\n"]},{"cell_type":"code","source":["\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"id":"6d_ARRTh6etE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqfDY2y0HzEs"},"outputs":[],"source":["def single_flat_LSTM_50d_100(headlines, bodies, GloVe_vectors):\n","\n","    #########################\n","    # PARAMETER DEFINITIONS #\n","    #########################\n","    method_name = \"single_flat_LSTM_50d_100\"\n","    # location path for features\n","    PARAM_DICT_FILENAME = method_name+\"_param_dict.pkl\"\n","\n","    param_dict = {\n","        \"MAX_NB_WORDS\": 50000,  # size of the vocabulary\n","\n","        # sequence lengths\n","        \"MAX_SEQ_LENGTH\": 100, #1000\n","\n","        # embedding specific values\n","        \"EMBEDDING_DIM\": 50,  # dimension of the GloVe embeddings\n","\n","        # embedding file names\n","        \"EMBEDDING_FILE\": method_name+\"_embedding.npy\",\n","\n","        # vocab file names\n","        \"VOCAB_FILE\": method_name+\"_vocab.pkl\",\n","    }\n","\n","\n","    ###############################################\n","    # GET VOCABULARY AND PREPARE EMBEDDING MATRIX #\n","    ###############################################\n","\n","    # load all claims, orig_docs and evidences\n","    all = headlines.copy()\n","    all.extend(bodies.copy())\n","\n","    # create and save the embedding matrices for claims, orig_docs and evidences\n","    vocab = create_embedding_lookup_pandas(all, param_dict[\"MAX_NB_WORDS\"], param_dict[\"EMBEDDING_DIM\"],\n","                                           GloVe_vectors, param_dict[\"EMBEDDING_FILE\"], param_dict[\"VOCAB_FILE\"], init_zeros=False,\n","                                           add_unknown=True, rdm_emb_init=True, tokenizer=nltk.word_tokenize)\n","\n","    # unload GloVe_vectors in order to make debugging possible\n","    del GloVe_vectors\n","\n","\n","    #################################################\n","    # Create sequences and embedding for the claims #\n","    #################################################\n","    print(\"Create sequences and embedding for the heads\")\n","\n","    concatenated = []\n","    for i in range(len(headlines)):\n","        concatenated.append(headlines[i] + \". \" + bodies[i])\n","\n","    # replace tokens of claims by vocabulary ids - the ids refer to the index of the embedding matrix which holds the word embedding for this vocab word\n","    sequences = text_to_sequences_fixed_size(concatenated, vocab, param_dict[\"MAX_SEQ_LENGTH\"], save_full_text=False,\n","                                             take_full_claim=True)\n","\n","\n","\n","    #################################################\n","    # SAVE PARAM_DICT AND CONCATENATE TRAINING DATA #\n","    #################################################\n","\n","    # save param_dict\n","    with open(FEATURES_DIR+PARAM_DICT_FILENAME, 'wb') as f:\n","        pickle.dump(param_dict, f, pickle.HIGHEST_PROTOCOL)\n","    print(\"Save PARAM_DICT as \" + FEATURES_DIR+PARAM_DICT_FILENAME)\n","\n","    return sequences"]},{"cell_type":"markdown","source":["# Generate Word Embedding Matrix"],"metadata":{"id":"W-w51juC1ajL"}},{"cell_type":"code","source":["%%time\n","\n","# load fasttext vectors from disk by glove library\n","# takes a little time...don't worry!\n","GloVe_vectors = load_embedding_pandas(fasttext_path)\n","\n","\n","# create a dataframe from glove vectors\n","# takes a little time...don't worry!\n","g_vec = pd.DataFrame.from_dict(GloVe_vectors)\n"],"metadata":{"id":"2exjkQs802OL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693062970073,"user_tz":-210,"elapsed":158190,"user":{"displayName":"Vahid Kiani","userId":"06632536328202809770"}},"outputId":"abdb75ac-41a7-4513-9dc6-3c59bc3e7b17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 2000000 word vectors.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXcSLm-SUmKh","executionInfo":{"status":"ok","timestamp":1693062970074,"user_tz":-210,"elapsed":28,"user":{"displayName":"Vahid Kiani","userId":"06632536328202809770"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b08e1051-d3cb-458d-a2b9-d4cf3b353657"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0      0.0471\n","1      0.0085\n","2      0.0203\n","3      0.0081\n","4      0.0089\n","        ...  \n","295   -0.0165\n","296   -0.0096\n","297    0.0135\n","298    0.0094\n","299   -0.0394\n","Name: اسلامی, Length: 300, dtype: float32"]},"metadata":{},"execution_count":16}],"source":["# print a sample word-vector\n","g_vec.iloc[:,200]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNEMxYF06BWf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693062970075,"user_tz":-210,"elapsed":21,"user":{"displayName":"Vahid Kiani","userId":"06632536328202809770"}},"outputId":"d8bbbd17-ef37-44e6-cb74-e1730c7a07e3"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":17}],"source":["nltk.download('punkt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0iYxaWtCXjCF","executionInfo":{"status":"ok","timestamp":1693062978198,"user_tz":-210,"elapsed":8134,"user":{"displayName":"Vahid Kiani","userId":"06632536328202809770"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6dee585-839e-435b-d11e-e8477aa02987"},"outputs":[{"output_type":"stream","name":"stdout","text":["can't find npy or pkl file!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["created embedding lookup!\n","embedding matrix saved!\n","vocab saved!\n","Embedding lookup table shape for single_flat_LSTM_50d_100_embedding.npy is: (48758, 50)\n","Vocab size for single_flat_LSTM_50d_100_vocab.pkl is: 48757\n","Create sequences and embedding for the heads\n"]},{"output_type":"stream","name":"stderr","text":["1997it [00:03, 516.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Save PARAM_DICT as /content/drive/MyDrive/Stance Detection Project/Baseline Zarharan/features/single_flat_LSTM_50d_100_param_dict.pkl\n","(1997, 100)\n","CPU times: user 7.41 s, sys: 34.6 ms, total: 7.45 s\n","Wall time: 7.7 s\n"]}],"source":["%%time\n","\n","# construct the required word embeddng matrix and embed words using fasttext library\n","# all dataset instances are embedded here with dimension of vectors: 50, max sequence length: 100\n","\n","# Reduction of Glove Word Embedding Model to Dimension of 50 instead of 300\n","# Generated File is Saved to Features Directory of the Project\n","# This File is Later Used by Feature Extraction Function\n","sequences = single_flat_LSTM_50d_100(clean_claim.to_list(), clean_body.to_list(), g_vec)   # dimension of vectors: 50, max sequence length: 100\n","\n","print(sequences.shape)"]},{"cell_type":"markdown","source":["# Save Sequences"],"metadata":{"id":"9218CbYIhCog"}},{"cell_type":"code","source":["# save sequences of token_ids to file\n","np.save(sequences_file, sequences)"],"metadata":{"id":"U__TUO6QfJnd"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["k6BwwgST7Mzw"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":0}