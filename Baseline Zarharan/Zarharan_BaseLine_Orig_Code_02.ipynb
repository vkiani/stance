{"cells":[{"cell_type":"markdown","metadata":{"id":"EAgNOJBVVMQ_"},"source":["# Persian Stance Classification - Deep Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMgbaBPU5wuC"},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import sys\n","import datetime\n","import argparse\n","import os\n","import csv\n","import numpy as np\n","import os.path as path"]},{"cell_type":"markdown","metadata":{"id":"FXQD-NMJrDS5"},"source":["# Mount Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5eNNmTGnZb4f","executionInfo":{"status":"ok","timestamp":1666336917603,"user_tz":-210,"elapsed":25138,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"f49da4e0-7336-4862-e0e8-b87c4af90819"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ecz6Z6D1nkaQ"},"outputs":[],"source":["# define path of input files\n","data_path = \"/content/drive/MyDrive/Stance Detection Project/ArticleToClaim.txt\"\n","stopwords_path = \"/content/drive/MyDrive/Stance Detection Project/StopWords_fa.txt\"\n","fasttext_path = \"/content/drive/MyDrive/Stance Detection Project/cc.fa.300.vec\"   # this is a text file\n","\n","# define path of output files\n","csv_path = \"/content/drive/MyDrive/Stance Detection Project/ArticleToClaim.csv\"\n","cleaned_path = \"/content/drive/MyDrive/Stance Detection Project/Clean_Claim_Body.csv\"\n","train_path = \"/content/drive/MyDrive/Stance Detection Project/train_data.csv\"\n","test_path = \"/content/drive/MyDrive/Stance Detection Project/test_data.csv\"\n","FEATURES_DIR = '/content/drive/MyDrive/Stance Detection Project/features/'\n","deep_model_path = '/content/drive/MyDrive/persian_stance_deep_data/models/model_v1.h5'"]},{"cell_type":"markdown","metadata":{"id":"VAvZfSsgZoRd"},"source":["# Prepare Dataset"]},{"cell_type":"markdown","metadata":{"id":"u4_82UHQrxaX"},"source":["### Read Dataset from Text File"]},{"cell_type":"markdown","metadata":{"id":"4pvBMbjyZ7EN"},"source":["Claim and Body\n","\n","1997 samples\n","\n","748\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjBiHamk6v4o","executionInfo":{"status":"ok","timestamp":1666336918502,"user_tz":-210,"elapsed":903,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"d7231ba1-61b4-4b6b-bc11-4ae4754af688"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of samples :  1997\n"]}],"source":["# read dataset from text file\n","import glob\n","\n","# read text file (ArticleToClaim.txt)\n","data_file = glob.glob(data_path)\n","if len(data_file) == 0:\n","  raise Exception('Data file not found at ' + data_path)\n","\n","row_documents = []\n","cnt = 1\n","\n","for file in data_file:\n","    with open (file, \"r\", encoding=\"utf-8\") as fp:\n","        line = fp.readline() #first line is for the headers (Claim, Body Text, Claim Is Question, Claim Has Tow Parts, Stance)\n","        content = fp.read() #read the rest of the file to a string\n","\n","row_documents = content.split(\"#@@@@@#\") #split the instances by the custom delimiter\n","row_documents = list(filter(None, row_documents)) #remove empty instance\n","print(\"number of samples : \", len(row_documents))"]},{"cell_type":"markdown","metadata":{"id":"NyEcdB9Cr8xf"},"source":["### Write Dataset to CSV File"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aOoWStpTGqf8","executionInfo":{"status":"ok","timestamp":1666336918910,"user_tz":-210,"elapsed":413,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"3d774f57-a81b-4f68-ca5a-5b7fdae6c6c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["converting text data to csv file..\n","(0, 'مسمومیت زا بودن شدید قارچهای خودرو (وحشی) مشابه قارچهای موجود در بازار !', 'به گزارش اقتصادآنلاین به نقل از تسنیم، رخداد مسمومیت قارچی بهار 97؛ حادثه\\u200cهای مرگباری بود که نه از طریق تصادف یا سقوط و یا غرق شدگی - برای مسافران و گردشگران رخ داده باشد؛ بلکه در رویدادی کم تکرار - علت این مرگ\\u200cها قارچ بوده است! بطوریکه تا  غروب دوشنبه 31 اردیبهشت 1017 نفر به بیمارستان\\u200cها مراجعه کردند که از این تعداد 116 نفر در بیمارستان بستری شدند و متأسفانه 15 نفر جان خود را از دست دادند.\\n\\nدر این زمینه برای واکاوی بیشتر این رویداد تلخ و تاسف بار که می\\u200cطلبید برخی نهادهای مسؤل در حوزه طبیعت از جمله سازمان محیط زیست و .... قبل از وقوع این حوادث که به زعم کارشناسان امر یکی از دلایل مهم آن بارندگی زیاد و رویش این قارچ\\u200cها بود، مردم نسبت به استفاده نکردن خوراکی این قارچ\\u200cها حداقل از نظر آگاهی رسانی مطلع و به آنان هشدار داده می\\u200cشد.', '0', '0', 'Discuss', 'Discuss')\n"]}],"source":["print('converting text data to csv file..')\n","claim = []\n","body = []\n","question = []\n","part = []\n","headline = []\n","label = []    \n","index = []\n","i = 0\n","row_doc = np.asarray(row_documents)\n","for row in row_doc:\n","    claim.append(row.split(',')[0])\n","    body.append(row.split(',')[1])\n","    question.append(row.split(',')[2])\n","    part.append(row.split(',')[3])\n","    headline.append(row.split(',')[4])\n","    label.append(row.split(',')[-1])\n","    index.append(i)\n","    i += 1\n","    \n","Dataset = list(zip(index, claim, body, question, part, headline, label))\n","print(Dataset[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZo7W-BrHPQB","executionInfo":{"status":"ok","timestamp":1666336919701,"user_tz":-210,"elapsed":793,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"6d12c533-0bea-4a9d-f3ec-c6ae89c44d02"},"outputs":[{"output_type":"stream","name":"stdout","text":["done!\n"]}],"source":["df = pd.DataFrame(data = Dataset, columns=['index', 'claim', 'body', 'question', 'part', 'headline', 'label'])\n","df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n","\n","print('done!')"]},{"cell_type":"markdown","metadata":{"id":"14UC0EVssP9Q"},"source":["### Read Dataset from CSV File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wTYhVbW5ovEh"},"outputs":[],"source":["dataset = pd.read_csv(csv_path, index_col = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25NC72fy3VV5"},"outputs":[],"source":["dataset = dataset.sort_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PWGYroc8aFa5","executionInfo":{"status":"ok","timestamp":1666336920307,"user_tz":-210,"elapsed":39,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"3a35f07e-6eae-429c-a6d9-2e0132d358b4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['claim', 'body', 'question', 'part', 'headline', 'label'], dtype='object')"]},"metadata":{},"execution_count":13}],"source":["dataset.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLIiViZiabv8"},"outputs":[],"source":["claim = dataset['claim']\n","headline = dataset['headline']\n","body = dataset['body']\n","label = dataset['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IT_sz_7fbYq-","executionInfo":{"status":"ok","timestamp":1666336920309,"user_tz":-210,"elapsed":38,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"23107f9f-ba33-4065-e089-53723948614d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1997"]},"metadata":{},"execution_count":15}],"source":["len(body)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JV2OY21RatRc","executionInfo":{"status":"ok","timestamp":1666336920309,"user_tz":-210,"elapsed":33,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"1862c79b-1ab0-4d4b-a282-a512914468f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["مسمومیت زا بودن شدید قارچهای خودرو (وحشی) مشابه قارچهای موجود در بازار ! \n"," به گزارش اقتصادآنلاین به نقل از تسنیم، رخداد مسمومیت قارچی بهار 97؛ حادثه‌های مرگباری بود که نه از طریق تصادف یا سقوط و یا غرق شدگی - برای مسافران و گردشگران رخ داده باشد؛ بلکه در رویدادی کم تکرار - علت این مرگ‌ها قارچ بوده است! بطوریکه تا  غروب دوشنبه 31 اردیبهشت 1017 نفر به بیمارستان‌ها مراجعه کردند که از این تعداد 116 نفر در بیمارستان بستری شدند و متأسفانه 15 نفر جان خود را از دست دادند.\n","\n","در این زمینه برای واکاوی بیشتر این رویداد تلخ و تاسف بار که می‌طلبید برخی نهادهای مسؤل در حوزه طبیعت از جمله سازمان محیط زیست و .... قبل از وقوع این حوادث که به زعم کارشناسان امر یکی از دلایل مهم آن بارندگی زیاد و رویش این قارچ‌ها بود، مردم نسبت به استفاده نکردن خوراکی این قارچ‌ها حداقل از نظر آگاهی رسانی مطلع و به آنان هشدار داده می‌شد. \n"," Discuss\n"]}],"source":["print(claim[0], \"\\n\", body[0], \"\\n\", label[0])"]},{"cell_type":"markdown","metadata":{"id":"YRBEf-DCscE3"},"source":["### Data Analyzes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":264},"id":"P8S0PS0VzZ1X","executionInfo":{"status":"ok","timestamp":1666336920310,"user_tz":-210,"elapsed":30,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"f66e210b-0269-4023-ac11-c17d66d23839"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 288x216 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["import matplotlib.pyplot as plt\n","\n","fig = plt.figure(figsize=(4,3))\n","dataset.groupby('label').headline.count().plot.bar(ylim=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jo637O3-zaMi","executionInfo":{"status":"ok","timestamp":1666336920311,"user_tz":-210,"elapsed":29,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"1995e2a4-416b-4007-db48-72167acaa86a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["label\n","Agree         137\n","Disagree      206\n","Discuss      1068\n","Unrelated     586\n","Name: headline, dtype: int64"]},"metadata":{},"execution_count":18}],"source":["dataset.groupby('label').headline.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVFvfaRAnGeK","executionInfo":{"status":"ok","timestamp":1666336920312,"user_tz":-210,"elapsed":25,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"3e9dc056-7698-4575-dad8-141ec922e346"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1997"]},"metadata":{},"execution_count":19}],"source":["len(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8ZJraSam5rG","executionInfo":{"status":"ok","timestamp":1666336920312,"user_tz":-210,"elapsed":22,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"168152a8-257d-4ec9-c7da-f880a570b9b1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["label\n","Agree         6.860290\n","Disagree     10.315473\n","Discuss      53.480220\n","Unrelated    29.344016\n","Name: headline, dtype: float64"]},"metadata":{},"execution_count":20}],"source":["dataset.groupby('label').headline.count() * 100/len(dataset)"]},{"cell_type":"markdown","metadata":{"id":"ibRE0Lj9tSMa"},"source":["# Clean Data"]},{"cell_type":"markdown","metadata":{"id":"hEaTOoUXzJBI"},"source":["### Clean Data by Hazm Library"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kNzkY8YQ2Hbb","executionInfo":{"status":"ok","timestamp":1666336932657,"user_tz":-210,"elapsed":12364,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"3dda2ba2-e36e-4682-a23f-1e28f6aecafb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting hazm\n","  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n","\u001b[K     |████████████████████████████████| 316 kB 5.1 MB/s \n","\u001b[?25hCollecting libwapiti>=0.2.1\n","  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n","\u001b[K     |████████████████████████████████| 233 kB 44.7 MB/s \n","\u001b[?25hCollecting nltk==3.3\n","  Downloading nltk-3.3.0.zip (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 49.8 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n","Building wheels for collected packages: nltk, libwapiti\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394488 sha256=c486fc981f5c45ea23d7730c46279f9020aad15a8719266be2b9794385001b1b\n","  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n","  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154754 sha256=ba258ee6ffa72a7d1cee46988cdb7f411c73eb1c7bc7ee7d2d788dd0a7fff529\n","  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n","Successfully built nltk libwapiti\n","Installing collected packages: nltk, libwapiti, hazm\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.7\n","    Uninstalling nltk-3.7:\n","      Successfully uninstalled nltk-3.7\n","Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"]}],"source":["!pip install hazm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMU7fE5c1bIh","executionInfo":{"status":"ok","timestamp":1666336933802,"user_tz":-210,"elapsed":1159,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"58c47ece-7aee-486a-e321-4a47f16a4fa6"},"outputs":[{"output_type":"stream","name":"stdout","text":["hazm is already installed\n"]}],"source":["try:\n","    from hazm import *\n","    print('hazm is already installed')\n","except ImportError as e:\n","    raise Exception('hazm is not installed')\n","\n","# installation command:\n","# !pip install hazm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wakkSlcxUkt"},"outputs":[],"source":["from __future__ import unicode_literals\n","from hazm import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zdy5FjpMvUqZ"},"outputs":[],"source":[" \n","k = []\n","with open(stopwords_path, 'r', encoding=\"utf-8\") as f:\n","    for word in f:\n","        word = word.split('\\n')\n","        k.append(word[0])\n","        \n","def remove_stopwords(text):\n","    sw_data = []\n","    for i in text:\n","        for j in k:\n","            if j in word_tokenize(i):\n","                i.replace(j, '')\n","        sw_data.append(i)\n","    return sw_data\n","\n","\n","def remove_slash(text):\n","    ext_data = []\n","    for i in text:\n","        if '/' in i:\n","            spl = i.split('/')\n","            if 'شایعه' in spl[-1]:\n","                i = i.replace(spl[-1], '')\n","        ext_data.append(i)\n","    return ext_data\n","\n","\n","import re\n","r = re.compile(\"[\\!\\;,؟:?،؛.+»«<>|\\#(\\)\\-\\/\\'\\\"]\")\n","def remove_punc(text):\n","    punc = []\n","    for i in text:\n","        punc.append(r.sub(\"\", i))\n","    return punc\n","\n","extra_str = ['\\u200c', '\\u200d', '\\u200e', '\\u200b', '\\r', '\\n', '\\ufeff']\n","def clean_data(text):\n","    \n","    print(\"start cleaning data..\")\n","    \n","    text = remove_slash(text)\n","    \n","    clean_data = []\n","    for i in text:\n","        for j in extra_str:\n","            if j in i:\n","                i = i.replace(j,'')\n","        clean_data.append(i)\n","    \n","    print(\"data is ready!\")\n","    return clean_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZk9XZU7Q8U3"},"outputs":[],"source":["import re\n","r = re.compile(\"[\\!\\;,؟:?،؛.+»«<>|\\#(\\)\\-\\/\\'\\\"]\")\n","def clean(text):\n","    return r.sub(\"\", text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOeLIjB_wfa1","executionInfo":{"status":"ok","timestamp":1666336934207,"user_tz":-210,"elapsed":8,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"e776abdb-1342-4991-f4e1-c5adbd652d75"},"outputs":[{"output_type":"stream","name":"stdout","text":["start cleaning data..\n","data is ready!\n"]}],"source":["clean_claim = clean_data(claim)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKhhrM922IbE","executionInfo":{"status":"ok","timestamp":1666336934938,"user_tz":-210,"elapsed":737,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"97b7b18e-917c-471b-a656-0e615a44cd28"},"outputs":[{"output_type":"stream","name":"stdout","text":["start cleaning data..\n","data is ready!\n"]}],"source":["clean_body = clean_data(body)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"sHp4foE6yVx4","executionInfo":{"status":"ok","timestamp":1666336934939,"user_tz":-210,"elapsed":8,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"f789e894-19ca-4568-c1f8-3bae50bc77d6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'به گزارش اقتصادآنلاین به نقل از تسنیم، رخداد مسمومیت قارچی بهار 97؛ حادثههای مرگباری بود که نه از طریق تصادف یا سقوط و یا غرق شدگی - برای مسافران و گردشگران رخ داده باشد؛ بلکه در رویدادی کم تکرار - علت این مرگها قارچ بوده است! بطوریکه تا  غروب دوشنبه 31 اردیبهشت 1017 نفر به بیمارستانها مراجعه کردند که از این تعداد 116 نفر در بیمارستان بستری شدند و متأسفانه 15 نفر جان خود را از دست دادند.در این زمینه برای واکاوی بیشتر این رویداد تلخ و تاسف بار که میطلبید برخی نهادهای مسؤل در حوزه طبیعت از جمله سازمان محیط زیست و .... قبل از وقوع این حوادث که به زعم کارشناسان امر یکی از دلایل مهم آن بارندگی زیاد و رویش این قارچها بود، مردم نسبت به استفاده نکردن خوراکی این قارچها حداقل از نظر آگاهی رسانی مطلع و به آنان هشدار داده میشد.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}],"source":["clean_body[0]"]},{"cell_type":"markdown","metadata":{"id":"c_kkgLgYzPKx"},"source":["### Write Cleaned Data to CSV File"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M69ZHTh538Sb","executionInfo":{"status":"ok","timestamp":1666336935302,"user_tz":-210,"elapsed":370,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"d7e6ede8-3f26-461d-aa45-b3eb36dedd3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["save clean data.\n"]}],"source":["Dataset = list(zip(clean_claim, clean_body, label))\n","np.random.shuffle(Dataset)  \n","df = pd.DataFrame(data = Dataset, columns=['claim', 'body', 'label'])\n","df.to_csv(cleaned_path, index=True, encoding=\"utf-8\")\n","\n","print('save clean data.')"]},{"cell_type":"markdown","metadata":{"id":"TU7zcU39zUQw"},"source":["### Read Cleaned Data from CSV File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fa_zU2y2vD_K"},"outputs":[],"source":["import pandas as pd\n","dataset_clean = pd.read_csv(cleaned_path, index_col = 0, )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUhHHXho1a9n"},"outputs":[],"source":["clean_claim = dataset_clean['claim']\n","clean_body = dataset_clean['body']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"rJUbSCMfvUqC","executionInfo":{"status":"ok","timestamp":1666336935307,"user_tz":-210,"elapsed":19,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"0eb12b80-de65-4e4f-b9d0-23e8ccb9eb93"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               claim  \\\n","0     بازی عجیب نماینده چابهار در مجلس با میکروفون !   \n","1  مسمومیت زا بودن شدید قارچهای خودرو (وحشی) مشاب...   \n","2  تذکر عباسپور در لحظه قهرمانی به یک دختر، برای ...   \n","3            تخریب تندیس شهریار در پارسآباد اردبیل !   \n","4  ساخت میدانی به نام صدام، پاسخ کمک های ایران به...   \n","\n","                                                body      label  \n","0  به گزارش ایسنا- عبدالغفور ایران نژاد افزود: 10...  Unrelated  \n","1  سخنگوی اورژانس کشور در گفت و گو با خبرنگار گرو...    Discuss  \n","2  به گزارش سایت فدراسیون بدنسازی و پرورش اندام، ...  Unrelated  \n","3  به گزارش آران مغان، این تندیس که 10 فروردين ما...      Agree  \n","4  به گزارش«شیعه نیوز»، سرلشکر رافع رواجبه استاند...    Discuss  "],"text/html":["\n","  <div id=\"df-0c944989-fd0f-4c56-bc61-e670cd55960e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>claim</th>\n","      <th>body</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>بازی عجیب نماینده چابهار در مجلس با میکروفون !</td>\n","      <td>به گزارش ایسنا- عبدالغفور ایران نژاد افزود: 10...</td>\n","      <td>Unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>مسمومیت زا بودن شدید قارچهای خودرو (وحشی) مشاب...</td>\n","      <td>سخنگوی اورژانس کشور در گفت و گو با خبرنگار گرو...</td>\n","      <td>Discuss</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>تذکر عباسپور در لحظه قهرمانی به یک دختر، برای ...</td>\n","      <td>به گزارش سایت فدراسیون بدنسازی و پرورش اندام، ...</td>\n","      <td>Unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>تخریب تندیس شهریار در پارسآباد اردبیل !</td>\n","      <td>به گزارش آران مغان، این تندیس که 10 فروردين ما...</td>\n","      <td>Agree</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ساخت میدانی به نام صدام، پاسخ کمک های ایران به...</td>\n","      <td>به گزارش«شیعه نیوز»، سرلشکر رافع رواجبه استاند...</td>\n","      <td>Discuss</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c944989-fd0f-4c56-bc61-e670cd55960e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0c944989-fd0f-4c56-bc61-e670cd55960e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0c944989-fd0f-4c56-bc61-e670cd55960e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":32}],"source":["dataset_clean.head()"]},{"cell_type":"markdown","metadata":{"id":"JYtjjx2bzfRJ"},"source":["#Split Data"]},{"cell_type":"markdown","metadata":{"id":"kIhqQoUd0MgA"},"source":["### Split Dataset into Train and Test sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fL2Q4zNQvpT5"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","y = dataset_clean.label\n","X = dataset_clean.drop('label', axis=1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"ERs3eyYm0R-Q"},"source":["### Save Data Train to CSV File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_ur1L11xOJL"},"outputs":[],"source":["# Data_train = list(zip(X_train['claim'], X_train['body'], y_train))\n","    \n","# df = pd.DataFrame(data = Data_train, columns=['claim', 'body', 'label'])\n","# df.to_csv(train_path, index=True, encoding=\"utf-8\")\n","\n","# print('save train data.')"]},{"cell_type":"markdown","metadata":{"id":"6cBz8F-t0Vdo"},"source":["### Save Data Test to CSV File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atY7I1cxy_6C"},"outputs":[],"source":["# Data_test = list(zip(X_test['claim'], X_test['body'], y_test))\n","    \n","# df = pd.DataFrame(data = Data_test, columns=['claim', 'body', 'label'])\n","# df.to_csv(test_path, index=True, encoding=\"utf-8\")\n","\n","# print('save test data.')"]},{"cell_type":"markdown","metadata":{"id":"mNnlE1OT0ZS4"},"source":["### Read Data Train and Test from CSV Files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvvkjOz0ynTI"},"outputs":[],"source":["data_train = pd.read_csv(train_path, index_col = 0, )\n","data_test = pd.read_csv(test_path, index_col = 0, )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ThVeCTtnysw-","executionInfo":{"status":"ok","timestamp":1666336935946,"user_tz":-210,"elapsed":16,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"ffbe5625-ed70-4477-b643-d6fc1032bde9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1597, 400)"]},"metadata":{},"execution_count":37}],"source":["len(data_train), len(data_test)"]},{"cell_type":"markdown","metadata":{"id":"u5kmac9sZtlM"},"source":["# Extract Feature"]},{"cell_type":"markdown","metadata":{"id":"lVM3qiwYiytW"},"source":["### BOW Feature Extractor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SBbHZ8W6bND6","outputId":"ba4d3c09-c3af-46e1-9644-c7e617bc9660"},"outputs":[{"output_type":"stream","name":"stderr","text":["1997it [00:00, 228391.37it/s]\n"]}],"source":["def ngrams(input, n):\n","    input = input.split(' ')\n","    output = []\n","    for i in range(len(input) - n + 1):\n","        output.append(input[i:i + n])\n","    return output\n","\n","\n","def chargrams(input, n):\n","    output = []\n","    for i in range(len(input) - n + 1):\n","        output.append(input[i:i + n])\n","    return output\n","\n","\n","def append_chargrams(features, text_headline, text_body, size):\n","    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(text_headline.split())), size)]\n","    grams_hits = 0\n","    grams_early_hits = 0\n","    grams_first_hits = 0\n","    for gram in grams:\n","        if gram in text_body:\n","            grams_hits += 1\n","        if gram in text_body[:255]:\n","            grams_early_hits += 1\n","        if gram in text_body[:100]:\n","            grams_first_hits += 1\n","    features.append(grams_hits)\n","    features.append(grams_early_hits)\n","    features.append(grams_first_hits)\n","    return features\n","\n","\n","def append_ngrams(features, text_headline, text_body, size):\n","    grams = [' '.join(x) for x in ngrams(text_headline, size)]\n","    grams_hits = 0\n","    grams_early_hits = 0\n","    for gram in grams:\n","        if gram in text_body:\n","            grams_hits += 1\n","        if gram in text_body[:255]:\n","            grams_early_hits += 1\n","    features.append(grams_hits)\n","    features.append(grams_early_hits)\n","    return features\n","\n","\n","def hand_features(headlines, bodies):\n","\n","    def binary_co_occurence(headline, body):\n","        # Count how many times a token in the title\n","        # appears in the body text.\n","        bin_count = 0\n","        bin_count_early = 0\n","        for headline_token in clean(headline).split(\" \"):\n","            if headline_token in clean(body):\n","                bin_count += 1\n","            if headline_token in clean(body)[:255]:\n","                bin_count_early += 1\n","        return [bin_count, bin_count_early]\n","\n","    def binary_co_occurence_stops(headline, body):\n","        # Count how many times a token in the title\n","        # appears in the body text. Stopwords in the title\n","        # are ignored.\n","        bin_count = 0\n","        bin_count_early = 0\n","        for headline_token in remove_stopwords(clean(headline).split(\" \")):\n","            if headline_token in clean(body):\n","                bin_count += 1\n","                bin_count_early += 1\n","        return [bin_count, bin_count_early]\n","\n","    def count_grams(headline, body):\n","        # Count how many times an n-gram of the title\n","        # appears in the entire body, and intro paragraph\n","\n","        clean_body = clean(body)\n","        clean_headline = clean(headline)\n","        features = []\n","        features = append_chargrams(features, clean_headline, clean_body, 2)\n","        features = append_chargrams(features, clean_headline, clean_body, 8)\n","        features = append_chargrams(features, clean_headline, clean_body, 4)\n","        features = append_chargrams(features, clean_headline, clean_body, 16)\n","        features = append_ngrams(features, clean_headline, clean_body, 2)\n","        features = append_ngrams(features, clean_headline, clean_body, 3)\n","        features = append_ngrams(features, clean_headline, clean_body, 4)\n","        features = append_ngrams(features, clean_headline, clean_body, 5)\n","        features = append_ngrams(features, clean_headline, clean_body, 6)\n","        return features\n","\n","    X = []\n","    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n","        X.append(binary_co_occurence(headline, body)\n","                 + binary_co_occurence_stops(headline, body)\n","                 + count_grams(headline, body))\n","    return X\n","# ------------------------------------------------------------------------------\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def char_3grams_5000_concat_all_data(headlines, bodies):\n","\n","    def combine_head_and_body(headlines, bodies):\n","        return [headline + \" \" + body for i, (headline, body) in\n","                tqdm(enumerate(zip(headlines, bodies)))]\n","\n","    # Load train data into CountVectorizer, get the resulting X-values and also the vocabulary\n","    # for the test data feature creation\n","    def get_features(headlines, bodies, headlines_all, bodies_all):\n","        # create vocab on basis of training data\n","        head_and_body = combine_head_and_body(headlines_all, bodies_all)\n","        head_and_body_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), lowercase=True,\n","                                              max_features=5000, use_idf=False, norm='l2')\n","        head_and_body_tfidf.fit(head_and_body)\n","        vocab = head_and_body_tfidf.vocabulary_\n","\n","        # create training feature vectors\n","        X_train_head_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), lowercase=True,\n","                                             stop_words='english', vocabulary=vocab, use_idf=False, norm='l2')\n","        X_train_head = X_train_head_tfidf.fit_transform(headlines)\n","\n","        X_train_body_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), lowercase=True,\n","                                             stop_words='english', vocabulary=vocab, use_idf=False, norm='l2')\n","        X_train_body = X_train_body_tfidf.fit_transform(bodies)\n","\n","        X_train = np.concatenate([X_train_head.toarray(), X_train_body.toarray()], axis=1)\n","\n","        return X_train\n","\n","    h, b = data_train['claim'].tolist() , data_train['body'].tolist()\n","    h_test, b_test = data_test['claim'].tolist(), data_test['body'].tolist()\n","\n","    # Comment out for clean ablation tests\n","    h.extend(h_test)\n","    b.extend(b_test)\n","\n","    X_train = get_features(headlines, bodies, h, b)\n","\n","    return X_train\n","# ------------------------------------------------------------------------------\n","\n","# test this feature extractor function before final feature extraction step\n","x = char_3grams_5000_concat_all_data(clean_claim, clean_body)\n","# ------------------------------------------------------------------------------\n","print(x.shape)"]},{"cell_type":"markdown","metadata":{"id":"0dTO6k2uG8Rp"},"source":["### Negated Context Feature Extractor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXmoLmIlb6Z6"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# this function takes a very long time to finish execution!\n","def negated_context_word_12grams_concat_tf5000_l2_all_data(headlines, bodies):\n","    \"\"\"\n","    Negates string after special negation word by adding a \"NEG_\" in front\n","    of every negated word, until a punctuation mark appears.\n","    Source:\n","        NRC-Canada: Buidling the State-of-the-Art in Sentiment Analysis of Tweets\n","        http://sentiment.christopherpotts.net/lingstruc.html\n","        http://stackoverflow.com/questions/23384351/how-to-add-tags-to-negated-words-in-strings-that-follow-not-no-and-never\n","\n","\n","    :param headlines:\n","    :param bodies:\n","    :return:\n","    \"\"\"\n","    def get_negated_text(text):\n","      sens = text.replace(';','.').replace(',','.').replace('!','.').replace(':','.').replace('،', '').split('.')\n","      li_1 = ['هیچ', 'اصلا', 'هیچگونه']\n","      li_2 = [ 'ندارد', 'نمیتواند']\n","      jomles = []\n","      for sen in sens: \n","        first, second = 0 , 0\n","        flag_1, flag_2 = False, False\n","        tokens = word_tokenize(sen)\n","        jomle = []    \n","        for i in range(len(tokens)):\n","          if tokens[i] in li_1 and flag_1 == False:\n","            first = i\n","            flag_1 = True\n","          if tokens[i] in li_2 and flag_2 == False:\n","            second = i\n","            flag_2 = True\n","        if (second > first) and (flag_1 == True) and (flag_2 == True):\n","          for j in range (first + 1 , second-1 ):\n","            sen = sen.replace(tokens[j], 'NEG_'+tokens[j])\n","        jomles.append(sen)\n","    \n","      jomles = '. '.join(jomles)\n","  \n","      return jomles\n","\n","    def combine_head_and_body(headlines, bodies):\n","        head_and_body = [headline + \" \" + body for i, (headline, body) in\n","                         enumerate(zip(headlines, bodies))]\n","\n","        return head_and_body\n","\n","    def get_vocab(neg_headlines, neg_bodies):\n","        neg_headlines = remove_stopwords(neg_headlines)\n","        neg_bodies = remove_stopwords(neg_bodies)\n","        tf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, use_idf=False,\n","                                        norm='l2')\n","        tf_vectorizer.fit_transform(combine_head_and_body(neg_headlines, neg_bodies))\n","        vocab = tf_vectorizer.vocabulary_\n","\n","        return vocab\n","\n","    def get_features(neg_headlines_test, neg_bodies_test, vocab):\n","        neg_headlines_test = remove_stopwords(neg_headlines_test)\n","        neg_bodies_test = remove_stopwords(neg_bodies_test)\n","        \n","        tf_vectorizer_head = TfidfVectorizer(vocabulary=vocab, use_idf=False, norm='l2')\n","        X_test_head = tf_vectorizer_head.fit_transform(neg_headlines_test)\n","\n","        tf_vectorizer_body = TfidfVectorizer(vocabulary=vocab, use_idf=False, norm='l2')\n","        X_test_body = tf_vectorizer_body.fit_transform(neg_bodies_test)\n","\n","        X_test = np.concatenate([X_test_head.toarray(), X_test_body.toarray()], axis=1)\n","        return X_test\n","\n","#     h, b = get_head_body_tuples(include_holdout=True)\n","#     h_test, b_test = get_head_body_tuples_test()\n","    \n","    h, b = data_train['claim'].tolist() , data_train['body'].tolist()\n","    h_test, b_test = data_test['claim'].tolist(), data_test['body'].tolist()\n","\n","    # Comment out for clean ablation tests\n","    h.extend(h_test)\n","    b.extend(b_test)\n","\n","    neg_headlines_all = [get_negated_text(h) for h in h]\n","    neg_bodies_all = [get_negated_text(b) for b in b]\n","    neg_headlines = [get_negated_text(h) for h in headlines]\n","    neg_bodies = [get_negated_text(b) for b in bodies]\n","\n","    vocab = get_vocab(neg_headlines_all, neg_bodies_all)\n","    X_train = get_features(neg_headlines, neg_bodies, vocab)\n","\n","    return X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0KlrvAVTrQ-R"},"outputs":[],"source":["# x = negated_context_word_12grams_concat_tf5000_l2_all_data(clean_claim, clean_body)\n","# this line is commented in the main notebook"]},{"cell_type":"markdown","metadata":{"id":"yPN7xgRFi2V-"},"source":["### Create Word Embedding Matrix"]},{"cell_type":"markdown","metadata":{"id":"k6BwwgST7Mzw"},"source":["#### Download Fasttext Word Vectors File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBQdiEfv4WLO"},"outputs":[],"source":["# download fasttext word vectors for persian (text file version)\n","# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz \n","# ------------------------------------------------------------------------------\n","# unzip fasttext word vectors for persian\n","# !gunzip cc.fa.300.vec.gz\n","\n","# then copy unziped text file to you google drive for future use"]},{"cell_type":"markdown","metadata":{"id":"r47z4bBZ7Tso"},"source":["#### Load Embedding Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BQ1bFu7UBdk"},"outputs":[],"source":["def load_embedding_pandas(FILE, type=\"w2v\"):\n","  embeddings_index=dict()\n","  f = open(FILE)\n","  for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","  f.close()\n","  print('Loaded %s word vectors.' % len(embeddings_index))\n","  return embeddings_index\n","\n","# ------------------------------------------------------------------------------\n","\n","GloVe_vectors = load_embedding_pandas(fasttext_path)\n","# takes a little time...don't worry!\n","\n","# ------------------------------------------------------------------------------\n","\n","g_vec = pd.DataFrame.from_dict(GloVe_vectors)\n","# takes a little time...don't worry!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXcSLm-SUmKh"},"outputs":[],"source":["# print a sample word-vector\n","g_vec.iloc[:,200]"]},{"cell_type":"markdown","metadata":{"id":"vY4MwGZHl64v"},"source":["#### Remve feature for get better f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-_6n1lGl5FV"},"outputs":[],"source":["# char_3grams_5000_concat_all_data = FEATURES_DIR+'char_3grams_5000_concat_all_data.first_1.npy';\n","# hand = FEATURES_DIR+'hand.first_1.npy';\n","# negated_context_word_12grams_concat_tf5000_l2_all_data = FEATURES_DIR+'negated_context_word_12grams_concat_tf5000_l2_all_data.first_1.npy';\n","# single_flat_LSTM_50d_100_embedding = FEATURES_DIR+'single_flat_LSTM_50d_100_embedding.npy';\n","# single_flat_LSTM_50d_100_param_dict = FEATURES_DIR+'single_flat_LSTM_50d_100_param_dict.pkl';\n","# single_flat_LSTM_50d_100_vocab = FEATURES_DIR+'single_flat_LSTM_50d_100_vocab.pkl';\n","# single_flat_LSTM_50d_100 = FEATURES_DIR+'single_flat_LSTM_50d_100.first_1.npy';\n","\n","# try:\n","#   if char_3grams_5000_concat_all_data:\n","#     os.remove(char_3grams_5000_concat_all_data)\n","#   if hand:\n","#     os.remove(hand)\n","#   if negated_context_word_12grams_concat_tf5000_l2_all_data:\n","#     os.remove(negated_context_word_12grams_concat_tf5000_l2_all_data)\n","#   if single_flat_LSTM_50d_100_embedding:\n","#     os.remove(single_flat_LSTM_50d_100_embedding)\n","#   if single_flat_LSTM_50d_100_param_dict:\n","#     os.remove(single_flat_LSTM_50d_100_param_dict)\n","#   if single_flat_LSTM_50d_100_vocab:\n","#     os.remove(single_flat_LSTM_50d_100_vocab)\n","#   if single_flat_LSTM_50d_100:\n","#     os.remove(single_flat_LSTM_50d_100)\n","# except:\n","#   print(\"...\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2HgVqF9l4uo"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"z2dMrV2_90Jo"},"source":["#### Reduction of FastText from 300 to 50 Dimension Vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3CQB21S_37B"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5skj76c3apYu"},"outputs":[],"source":["import pickle\n","import nltk\n","\n","def create_embedding_lookup_pandas(text_list, max_nb_words, embedding_dim, embedding,\n","                            embedding_lookup_name, embedding_vocab_name, rdm_emb_init=False, add_unknown=False, tokenizer=None, init_zeros = False):\n","    \"\"\"\n","    Creates the claim embedding lookup table if it not already exists and returns the vocabulary for it\n","    :param text_list:\n","    :param max_nb_words:\n","    :param embedding_dim:\n","    :param GloVe_vectors:\n","    :param embedding_lookup_name:\n","    :param embedding_vocab_name:\n","    :return:\n","    \"\"\"\n","    #del GloVe_vectors\n","\n","    # if ...embedding.npy or vocab.pkl files don't exist: \n","    if not path.exists(FEATURES_DIR + embedding_lookup_name) or not path.exists(FEATURES_DIR + embedding_vocab_name):\n","        print(\"can't find npy or pkl file!\")\n","        vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=None, tokenizer=tokenizer,\n","                                            max_features=max_nb_words, use_idf=True)\n","        vectorizer.fit_transform(text_list)\n","        vocab = vectorizer.vocabulary_\n","\n","\n","        # do not use 0 since we want to use masking in the LSTM later on\n","        for word in vocab.keys():\n","            vocab[word] += 1\n","        if add_unknown == True:\n","            max_index = max(vocab.values())\n","            vocab[\"UNKNOWN\"] = max_index+1\n","\n","        # prepare embedding - create matrix that holds the glove vector for each vocab entry\n","        if rdm_emb_init == True:\n","            embedding_lookup = np.random.random((len(vocab) + 1, embedding_dim))\n","            zero_vec = np.zeros((embedding_dim))\n","            embedding_lookup[0] = zero_vec # for masking\n","        else:\n","            embedding_lookup = np.zeros((len(vocab) + 1, embedding_dim))\n","\n","        if init_zeros == False:\n","            for word, i in vocab.items():\n","                if word == \"UNKNOWN\":\n","                    embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n","                    #print(embedding_vector)\n","                else:\n","                    try:\n","                        embedding_vector = embedding.loc[word].as_matrix()\n","                    except KeyError: #https://stackoverflow.com/questions/15653966/ignore-keyerror-and-continue-program\n","                        continue\n","                if embedding_vector is not None:\n","                    # words not found in embedding index will be all-zeros.\n","                    embedding_lookup[i] = embedding_vector\n","        print(\"created embedding lookup!\")\n","        #print(embedding_lookup[-1])\n","        # save embedding matrix\n","        np.save(FEATURES_DIR + embedding_lookup_name, embedding_lookup)\n","        print(\"embedding matrix saved!\")\n","        # save vocab\n","        with open(FEATURES_DIR + embedding_vocab_name, 'wb') as f:\n","            pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n","        print(\"vocab saved!\")\n","\n","        print(\"Embedding lookup table shape for \" + embedding_lookup_name + \" is: \" + str(embedding_lookup.shape))\n","    #if both .npy and .pkl files exist:\n","    else:\n","        print(\"found npy and pkl files!\")\n","        with open(FEATURES_DIR + embedding_vocab_name, \"rb\") as f:\n","            vocab = pickle.load(f)\n","\n","    print(\"Vocab size for \" + embedding_vocab_name + \" is: \" + str(len(vocab)))\n","\n","    return vocab\n","\n","# ------------------------------------------------------------------------------\n","\n","nltk.download('punkt')\n","\n","# ------------------------------------------------------------------------------\n","\n","def text_to_sequences_fixed_size(texts, vocab, MAX_SENT_LENGTH, save_full_text=False, take_full_claim = False):\n","    \"\"\"\n","    Turns sentences of claims into sequences of indices provided by the given vocab.\n","    Unknown words will get an extra index, if\n","    the vocab has a token \"UNKNOWN\". The method takes the longest sentence of the claims, if the\n","    claim should have more than one sentence.\n","    :param texts:\n","    :param vocab:\n","    :param MAX_SENT_LENGTH:\n","    :return:\n","    \"\"\"\n","    data = np.zeros((len(texts), MAX_SENT_LENGTH), dtype='int32')\n","\n","    claims = []\n","    if take_full_claim == False:\n","        for claim in texts:\n","            claim_sents = nltk.sent_tokenize(claim)\n","            word_count_fct = lambda sentence: len(nltk.word_tokenize(sentence)) # take longest sentence of claim if it has more than one\n","            claims.append(max(claim_sents, key=word_count_fct))\n","    else:\n","        claims = texts\n","\n","    data_string_dict = {}\n","    for j, claim in tqdm(enumerate(claims)):\n","        claim_tokens = nltk.word_tokenize(claim.lower())\n","\n","        data_string = \"\"\n","        if save_full_text == True:\n","            for token in claim_tokens:\n","                data_string += token + \" \"\n","            data_string = data_string[:-1]\n","            data_string_dict[j] = data_string\n","\n","        for i, token in enumerate(claim_tokens):\n","            if i < MAX_SENT_LENGTH:\n","                index = vocab.get(token, \"UNKNOWN\")\n","                if index == \"UNKNOWN\":\n","                    index = vocab.get(index, None)\n","                if index != None:\n","                    data[j, i] = index\n","\n","    if save_full_text == True:\n","        return data, data_string_dict\n","    else:\n","        return data\n","\n","# ------------------------------------------------------------------------------\n","\n","def single_flat_LSTM_50d_100(headlines, bodies, GloVe_vectors):\n","\n","    #########################\n","    # PARAMETER DEFINITIONS #\n","    #########################\n","    method_name = \"single_flat_LSTM_50d_100\"\n","    # location path for features\n","    PARAM_DICT_FILENAME = method_name+\"_param_dict.pkl\"\n","\n","    param_dict = {\n","        \"MAX_NB_WORDS\": 50000,  # size of the vocabulary\n","\n","        # sequence lengths\n","        \"MAX_SEQ_LENGTH\": 100, #1000\n","\n","        # embedding specific values\n","        \"EMBEDDING_DIM\": 50,  # dimension of the GloVe embeddings\n","        #\"GLOVE_ZIP_FILE\": \"/content/drive/My Drive/persian_stance_baseline_data/vectors/cc.fa.300.vec.gz\",  #*********************\n","        #\"GLOVE_FILE\": \"/content/drive/My Drive/persian_stance_baseline_data/vectors/cc.fa.300.vec\",  #*********************\n","\n","        # embedding file names\n","        \"EMBEDDING_FILE\": method_name+\"_embedding.npy\",\n","\n","        # vocab file names\n","        \"VOCAB_FILE\": method_name+\"_vocab.pkl\",\n","    }\n","\n","\n","    ###############################################\n","    # GET VOCABULARY AND PREPARE EMBEDDING MATRIX #\n","    ###############################################\n","\n","    # load GloVe embeddings\n","    # load the whole embedding into memory\n","    \n","#     GloVe_vectors = load_embedding_pandas(param_dict[\"GLOVE_FILE\"])\n","    \n","\n","    # load all claims, orig_docs and evidences\n","    all_heads, all_bodies = data_train['claim'].tolist() , data_train['body'].tolist()\n","    all = all_heads\n","    all.extend(all_bodies)\n","   \n","    \n","\n","    # Comment out for clean ablation checks\n","    # add the unlabeled test data words to the BoW of test+train+holdout data\n","    h_unlbled_test, b_unlbled_test = data_test['claim'].tolist(), data_test['body'].tolist()\n","    all.extend(h_unlbled_test)\n","    all.extend(b_unlbled_test)\n","    \n","\n","    # create and save the embedding matrices for claims, orig_docs and evidences\n","    vocab = create_embedding_lookup_pandas(all, param_dict[\"MAX_NB_WORDS\"], param_dict[\"EMBEDDING_DIM\"],\n","                                           GloVe_vectors, param_dict[\"EMBEDDING_FILE\"], param_dict[\"VOCAB_FILE\"], init_zeros=False,\n","                                           add_unknown=True, rdm_emb_init=True, tokenizer=nltk.word_tokenize)\n","\n","    # unload GloVe_vectors in order to make debugging possible\n","    del GloVe_vectors\n","\n","\n","    #################################################\n","    # Create sequences and embedding for the claims #\n","    #################################################\n","    print(\"Create sequences and embedding for the heads\")\n","\n","    concatenated = []\n","    for i in range(len(headlines)):\n","        concatenated.append(headlines[i] + \". \" + bodies[i])\n","\n","    # replace tokens of claims by vocabulary ids - the ids refer to the index of the embedding matrix which holds the word embedding for this vocab word\n","    sequences = text_to_sequences_fixed_size(concatenated, vocab, param_dict[\"MAX_SEQ_LENGTH\"], save_full_text=False,\n","                                             take_full_claim=True)\n","\n","\n","\n","    #################################################\n","    # SAVE PARAM_DICT AND CONCATENATE TRAINING DATA #\n","    #################################################\n","\n","    # save param_dict\n","    with open(FEATURES_DIR+PARAM_DICT_FILENAME, 'wb') as f:\n","        pickle.dump(param_dict, f, pickle.HIGHEST_PROTOCOL)\n","    print(\"Save PARAM_DICT as \" + FEATURES_DIR+PARAM_DICT_FILENAME)\n","\n","    return sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9uMrbpUYE9m"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqfDY2y0HzEs"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNEMxYF06BWf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0iYxaWtCXjCF"},"outputs":[],"source":["# Reduction of Glove Word Embedding Model to Dimension of 50 instead of 300\n","# Generated File is Saved to Features Directory of the Project\n","# This File is Later Used by Feature Extraction Function\n","aa = single_flat_LSTM_50d_100(clean_claim, clean_body, g_vec)   # dimension of vectors: 50, max sequence length: 100"]},{"cell_type":"markdown","metadata":{"id":"oHFR3Mqyi-EF"},"source":["# Generate Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URLSI-k5h2OH"},"outputs":[],"source":["dataset_clean.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_Pz8dQUbElO"},"outputs":[],"source":["bow_feats = ['hand', 'negated_context_word_12grams_concat_tf5000_l2_all_data', 'char_3grams_5000_concat_all_data']\n","\n","word_emb = ['single_flat_LSTM_50d_100']\n","\n","#topic_models = ['latent_dirichlet_allocation_300', 'latent_semantic_indexing_gensim_300_concat', 'NMF_fit_all_concat_300_no_holdout', 'NMF_cos_300']\n","# topic_models = ['latent_dirichlet_allocation_300', 'latent_semantic_indexing_gensim_300_concat_holdout', 'NMF_fit_all_concat_300_and_test', 'NMF_cos_300']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9nfD-jiYDup"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"etKS3L3WhF58"},"outputs":[],"source":["def gen_or_load_feats(feat_fn, headlines, bodies, feature_file, feature):\n","    if not os.path.isfile(feature_file):\n","        if 'single_flat_LSTM_50d_100' in feature:\n","            feats = feat_fn(headlines, bodies, g_vec)\n","        else:\n","            feats = feat_fn(headlines, bodies)\n","        np.save(feature_file, feats)\n","\n","    return np.load(feature_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ohJHd6qgCDt"},"outputs":[],"source":["def generate_features(dataset, name, feature_list, features_dir):\n","    \"\"\"\n","    Creates feature vectors out of the provided dataset\n","    \"\"\"\n","    h, b, y = [], [], []\n","\n","    feature_dict = {'hand': hand_features,\n","                    'single_flat_LSTM_50d_100': single_flat_LSTM_50d_100,\n","                    'char_3grams_5000_concat_all_data': char_3grams_5000_concat_all_data,\n","                    'negated_context_word_12grams_concat_tf5000_l2_all_data': negated_context_word_12grams_concat_tf5000_l2_all_data,\n","                    }\n","    \n","    y = dataset['label'].tolist()\n","    h = dataset['claim'].tolist()\n","    b = dataset['body'].tolist()\n","\n","    X_feat = []\n","    feat_list = []\n","    last_index = 0\n","    for feature in feature_list:\n","        print(\"processing \" + feature)\n","        feat = gen_or_load_feats(feature_dict[feature], h, b, features_dir+\"/\"+feature+\".\"+name+'.npy', feature)\n","        feat_list.append((last_index, last_index+len(feat[0]), str(feature)))\n","        last_index += len(feat[0])\n","        X_feat.append(feat)\n","    print(\"done with processing all features in feature_list\")\n","    X = np.concatenate(X_feat, axis=1)\n","\n","    return X, y, feat_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3AzTfruhIhU"},"outputs":[],"source":["feature_list = word_emb + bow_feats\n","name = 'first_1'"]},{"cell_type":"markdown","metadata":{"id":"kv-16qHS8fMD"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHTazG37hJ_r"},"outputs":[],"source":["feature_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyBjbFoBhiMw"},"outputs":[],"source":["X, y , feature_list = generate_features(dataset_clean, name, feature_list, FEATURES_DIR)\n","# this code takes a very very long time to finish execution if the files required are not generated before...don't worry!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vsw4tRMW2Dhh"},"outputs":[],"source":["feature_list"]},{"cell_type":"markdown","metadata":{"id":"r69G5MZ-ZyNs"},"source":["# Base line (Majority Voting)"]},{"cell_type":"markdown","metadata":{"id":"d7M8JN4WaYwz"},"source":["majority vote"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtXP3tKXYlGF"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    dataset_clean['claim'], dataset_clean['label'], test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EppwBAJrXEMu"},"outputs":[],"source":["from sklearn.dummy import DummyClassifier\n","\n","clf_maj = DummyClassifier(strategy=\"most_frequent\")\n","\n","clf_maj.fit(X_train, y_train)\n","\n","y_pred_maj = clf_maj.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgFMeiAsg_Hl"},"outputs":[],"source":["LABELS = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n","LABELS_RELATED = ['unrelated','related']\n","RELATED = LABELS[0:3]\n","\n","def score_submission(gold_labels, test_labels):\n","    score = 0.0\n","    cm = [[0, 0, 0, 0],\n","          [0, 0, 0, 0],\n","          [0, 0, 0, 0],\n","          [0, 0, 0, 0]]\n","\n","    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n","        g_stance, t_stance = g, t\n","        if g_stance == t_stance:\n","            score += 0.25\n","            if g_stance != 'unrelated':\n","                score += 0.50\n","        if g_stance in RELATED and t_stance in RELATED:\n","            score += 0.25\n","\n","        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n","\n","    return score, cm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TW1IOOYwZKLi"},"outputs":[],"source":["score, cm = score_submission(y_test, y_pred_maj)\n","fold_score, _ = score_submission(y_test, y_pred_maj)\n","max_fold_score, _ = score_submission(y_test, y_test)\n","score = fold_score / max_fold_score\n","\n","print(\"FNC-1 score from restored model: \" +  str(score) +\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ylIEsw6ThrYz"},"outputs":[],"source":["def print_confusion_matrix(cm):\n","    lines = []\n","    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n","    line_len = len(header)\n","    lines.append(\"-\"*line_len)\n","    lines.append(header)\n","    lines.append(\"-\"*line_len)\n","\n","    hit = 0\n","    total = 0\n","    for i, row in enumerate(cm):\n","        hit += row[i]\n","        total += sum(row)\n","        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n","                                                                   *row))\n","        lines.append(\"-\"*line_len)\n","    print('\\n'.join(lines))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIn8-WFuYC3B"},"outputs":[],"source":["print_confusion_matrix(cm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avIijl1oj2qY"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","print( classification_report(y_test, y_pred_maj) )"]},{"cell_type":"markdown","metadata":{"id":"YrJQpoP8Z1xx"},"source":["# Deep Learning Model "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jm7gN_tDXQpF"},"outputs":[],"source":["def split_X(X_train, MAX_SEQ_LENGTH_HEADS):\n","    # split to get [heads, docs]\n","    X_train_splits = np.hsplit(X_train, np.array([MAX_SEQ_LENGTH_HEADS]))\n","    X_train_head = X_train_splits[0]\n","    X_train_doc = X_train_splits[1]\n","\n","    print(\"X_train_head.shape = \" + str(np.array(X_train_head).shape))\n","    print(\"X_train_doc.shape = \" + str(np.array(X_train_doc).shape))\n","\n","    return X_train_head,X_train_doc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxDn3uG42KQD"},"outputs":[],"source":["import numpy as np\n","import os.path as path\n","import pickle\n","\n","from keras.layers.core import Dense\n","# from keras.layers.pooling import GlobalMaxPooling1D\n","# from keras.layers.recurrent import LSTM\n","from keras.layers import LSTM\n","from keras import optimizers\n","# from fnc.models.Keras_utils import EarlyStoppingOnF1, convert_data_to_one_hot, calculate_class_weight, split_X\n","# from fnc.models.keras_custom_layers.attention_custom import *\n","from keras.models import Model, load_model\n","# from keras.layers.merge import concatenate\n","from keras.layers import concatenate\n","from keras.layers import Embedding, Input"]},{"cell_type":"code","source":["X.shape"],"metadata":{"id":"SN65CDwndhsZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BV_gZl423WL0"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kphZuuiS7xMe"},"outputs":[],"source":["def convert_data_to_one_hot(y_train):\n","    print(y_train)\n","    y_train_temp = np.zeros((y_train.size, y_train.max() + 1), dtype=np.int)\n","    y_train_temp[np.arange(y_train.size), y_train] = 1\n","\n","    return y_train_temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05-6Ix2L8EkU"},"outputs":[],"source":["from sklearn import preprocessing\n","import keras\n","\n","le = preprocessing.LabelEncoder()\n","le.fit(y_train)\n","print(list(le.classes_))\n","y_train = le.transform(y_train)\n","\n","le = preprocessing.LabelEncoder()\n","le.fit(y_test)\n","print(list(le.classes_))\n","y_test = le.transform(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNNYZbzY5Jjs"},"outputs":[],"source":["def calculate_class_weight(y_train, no_classes=2):\n","    # https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n","    from sklearn.utils import class_weight\n","\n","    class_weight_list = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n","    class_weights = {}\n","    for i in range(no_classes):\n","        class_weights[i] = class_weight_list[i]\n","    print(class_weights)\n","    return class_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3K_ngFR2Obp"},"outputs":[],"source":["y_train_one_hot = convert_data_to_one_hot(y_train)\n","y_test_one_hot = convert_data_to_one_hot(y_test)\n","\n","param_dict=\"single_flat_LSTM_50d_100\"\n","\n","PARAM_DICT_FILENAME = param_dict + \"_param_dict.pkl\"\n","\n","\n","# load feature dict for LSTM_1000_GloVe\n","with open(FEATURES_DIR+PARAM_DICT_FILENAME, \"rb\") as f:\n","  param_dict = pickle.load(f)\n","\n","# load parameters needed for embedding layer\n","EMBEDDING_DIM = param_dict[\"EMBEDDING_DIM\"] # e.g. 50\n","MAX_SEQ_LENGTH = param_dict[\"MAX_SEQ_LENGTH\"] # e.g. 100\n","\n","X_train_LSTM, X_train_MLP = split_X(X_train, MAX_SEQ_LENGTH)\n","X_test_LSTM, X_test_MLP = split_X(X_test, MAX_SEQ_LENGTH)\n","\n","# load embeddings\n","EMBEDDING_FILE = np.load(FEATURES_DIR+param_dict[\"EMBEDDING_FILE\"])\n","\n","print(\"EMBEDDING_FILE.shape = \" + str(EMBEDDING_FILE.shape))\n","\n","# calc cass weights\n","class_weights = calculate_class_weight(y_train, no_classes=4)"]},{"cell_type":"code","source":["X_train.shape"],"metadata":{"id":"ZySkZLXod6vu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_LSTM.shape"],"metadata":{"id":"GIfIcrZOd9lr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2ZdZpGm5TRs"},"outputs":[],"source":["kernel_initializer = 'glorot_uniform'\n","regularizer = None\n","batch_size=200\n","dense_activity_regularizer=None\n","LSTM_implementation = 2\n","\n","\n","################\n","# CLAIMS LAYER #\n","################\n","lstm_input = Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name='lstm_input') # receive sequences of MAX_SEQ_LENGTH_CLAIMS integers\n","embedding = Embedding(input_dim=len(EMBEDDING_FILE), # lookup table size\n","                                    output_dim=EMBEDDING_DIM, # output dim for each number in a sequence\n","                                    weights=[EMBEDDING_FILE],\n","                                    input_length=MAX_SEQ_LENGTH, # receive sequences of MAX_SEQ_LENGTH_CLAIMS integers\n","                                    mask_zero=True,\n","                                    trainable=True)(lstm_input)\n","data_LSTM = LSTM(\n","            100, return_sequences=True, stateful=False, dropout=0.2,\n","            batch_input_shape=(batch_size, MAX_SEQ_LENGTH, EMBEDDING_DIM),\n","            input_shape=(MAX_SEQ_LENGTH, EMBEDDING_DIM), implementation=LSTM_implementation\n","            )(embedding)\n","data_LSTM = LSTM(\n","            100, return_sequences=False, stateful=False, dropout=0.2,\n","            batch_input_shape=(batch_size, MAX_SEQ_LENGTH, EMBEDDING_DIM),\n","            input_shape=(MAX_SEQ_LENGTH, EMBEDDING_DIM), implementation=LSTM_implementation\n","            )(data_LSTM)\n","\n","###############################\n","# MLP (NON-TIMESTEP) FEATURES #\n","###############################\n","mlp_input = Input(shape=(len(X_train_MLP[0]),), dtype='float32', name='mlp_input')\n","\n","###############\n","# MERGE LAYER #\n","###############\n","merged = concatenate([data_LSTM, mlp_input])\n","\n","dense_mid = Dense(600, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n","                          activity_regularizer=dense_activity_regularizer, activation='relu')(merged)\n","dense_mid = Dense(600, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n","                          activity_regularizer=dense_activity_regularizer, activation='relu')(dense_mid)\n","dense_mid = Dense(600, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n","                          activity_regularizer=dense_activity_regularizer, activation='relu')(dense_mid)\n","dense_out = Dense(4,activation='softmax', name='dense_out')(dense_mid)\n","\n","# build model\n","model = Model(inputs=[lstm_input, mlp_input], outputs=[dense_out])\n","\n","# print summary\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qVH4fd1KDJzm"},"outputs":[],"source":["LABELS = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n","LABELS_RELATED = ['unrelated','related']\n","RELATED = LABELS[0:3]\n","\n","def score_submission(gold_labels, test_labels):\n","    score = 0.0\n","    cm = [[0, 0, 0, 0],\n","          [0, 0, 0, 0],\n","          [0, 0, 0, 0],\n","          [0, 0, 0, 0]]\n","\n","    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n","        g_stance, t_stance = g, t\n","        if g_stance == t_stance:\n","            score += 0.25\n","            if g_stance != 'unrelated':\n","                score += 0.50\n","        if g_stance in RELATED and t_stance in RELATED:\n","            score += 0.25\n","\n","        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n","\n","    return score, cm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzm0UREZC3fN"},"outputs":[],"source":["from keras.callbacks import Callback\n","import numpy as np\n","class EarlyStoppingOnF1(Callback):\n","    \"\"\"\n","    Prints some metrics after each epoch in order to observe overfitting\n","                https://github.com/fchollet/keras/issues/5794\n","                custom metrics: https://github.com/fchollet/keras/issues/2607\n","    \"\"\"\n","\n","    def __init__(self, epochs,\n","                 X_test_claims,\n","                 X_test_orig_docs,\n","                 y_test, loss_filename, epsilon=0.0, min_epoch = 15, X_test_nt=None):\n","        self.epochs = epochs\n","        self.patience = 2\n","        self.counter = 0\n","        self.prev_score = 0\n","        self.epsilon = epsilon\n","        self.loss_filename = loss_filename\n","        self.min_epoch = min_epoch\n","        self.X_test_nt = X_test_nt\n","        #self.print_train_f1 = print_train_f1\n","\n","        #self.X_train_claims = X_train_claims\n","        #self.X_train_orig_docs = X_train_orig_docs\n","        #self.X_train_evid = X_train_evid\n","        #self.y_train = y_train\n","\n","        self.X_test_claims = X_test_claims\n","        self.X_test_orig_docs = X_test_orig_docs\n","        self.y_test = y_test\n","        Callback.__init__(self)\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        if epoch + 1 < self.epochs:\n","            from sklearn.metrics import f1_score\n","\n","            # get prediction and convert into list\n","            if type(self.X_test_orig_docs).__module__ == np.__name__ and type(self.X_test_nt).__module__ == np.__name__:\n","                predicted_one_hot = self.model.predict([\n","                    self.X_test_claims,\n","                    self.X_test_orig_docs,\n","                    self.X_test_nt\n","                ])\n","            elif type(self.X_test_orig_docs).__module__ == np.__name__:\n","                predicted_one_hot = self.model.predict([\n","                    self.X_test_claims,\n","                    self.X_test_orig_docs,\n","                ])\n","            else:\n","                predicted_one_hot = self.model.predict(self.X_test_claims)\n","            predict = np.argmax(predicted_one_hot, axis=-1)\n","\n","            \"\"\"\n","            predicted_one_hot_train = self.model.predict([self.X_train_claims, self.X_train_orig_docs, self.X_train_evid])\n","            predict_train = np.argmax(predicted_one_hot_train, axis=-1)\n","\n","            \n","            # f1 for train data\n","            f1_macro_train = \"\"\n","            if self.print_train_f1 == True:\n","                f1_0_train = f1_score(self.y_train, predict_train, labels=[0], average=None)\n","                f1_1_train = f1_score(self.y_train, predict_train, labels=[1], average=None)\n","                f1_macro_train = (f1_0_train[0] + f1_1_train[0]) / 2\n","                print(\" - train_f1_(macro): \" + str(f1_macro_train))\"\"\"\n","\n","            predicted = [LABELS[int(a)] for a in predict]\n","            actual = [LABELS[int(a)] for a in self.y_test]\n","            # calc FNC score\n","            fold_score, _ = score_submission(actual, predicted)\n","            max_fold_score, _ = score_submission(actual, actual)\n","            fnc_score = fold_score / max_fold_score\n","            print(\" - fnc_score: \" + str(fnc_score))\n","\n","            # f1 for test data\n","            f1_0 = f1_score(self.y_test, predict, labels=[0], average=None)\n","            f1_1 = f1_score(self.y_test, predict, labels=[1], average=None)\n","            f1_2 = f1_score(self.y_test, predict, labels=[2], average=None)\n","            f1_3 = f1_score(self.y_test, predict, labels=[3], average=None)\n","            f1_macro = (f1_0[0] + f1_1[0] + f1_2[0] + f1_3[0]) / 4\n","            print(\" - val_f1_(macro): \" + str(f1_macro))\n","            print(\"\\n\")\n","\n","            header = \"\"\n","            values = \"\"\n","            for key, value in logs.items():\n","                header = header + key + \";\"\n","                values = values + str(value) + \";\"\n","            if epoch == 0:\n","                values = \"\\n\" + header + \"val_f1_macro;\" + \"fnc_score;\" + \"\\n\" + values + str(f1_macro) + str(fnc_score) + \";\"\n","            else:\n","                values += str(f1_macro) + \";\" + str(fnc_score) + \";\"\n","            append_to_loss_monitor_file(values, self.loss_filename)\n","\n","            if epoch >= self.min_epoch-1:  # 9\n","                if f1_macro + self.epsilon <= self.prev_score:\n","                    self.counter += 1\n","                else:\n","                    self.counter = 0\n","                if self.counter >= 2:\n","                    self.model.stop_training = True\n","            #print(\"Counter at \" + str(self.counter))\n","            self.prev_score = f1_macro\n","            #print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubk3uR3FF87-"},"outputs":[],"source":["def append_to_loss_monitor_file(text, filepath):\n","    with open(filepath, 'a+') as the_file:\n","        the_file.write(text+\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xn-ek0kr-wPk"},"outputs":[],"source":["from tensorflow.keras import optimizers\n","\n","lr = 0.001\n","optimizer_name = \"RMS\"\n","use_class_weights = True\n","epochs = 70\n","min_epoch = 20\n","save_folder = None\n","loss_filename = 'loss.h5'\n","\n","# optimizers\n","if optimizer_name == \"adagrad\":\n","  optimizer = optimizers.Adagrad(lr=lr)\n","  print(\"Used optimizer: adagrad, lr=\"+str(lr))\n","elif optimizer_name == \"adamax\":\n","  optimizer = optimizers.Adamax(lr=lr)\n","  print(\"Used optimizer: adamax, lr=\"+str(lr))\n","elif optimizer_name == \"nadam\":\n","  optimizer = optimizers.Nadam(lr=lr)  # recommended to leave at default params\n","  print(\"Used optimizer: nadam, lr=\"+str(lr))\n","elif optimizer_name == \"rms\":\n","  optimizer = optimizers.RMSprop(lr=lr)  # recommended for RNNs\n","  print(\"Used optimizer: rms, lr=\"+str(lr))\n","elif optimizer_name == \"SGD\":\n","  optimizer = optimizers.SGD(lr=lr)  # recommended for RNNs\n","  print(\"Used optimizer: SGD, lr=\"+str(lr))\n","elif optimizer_name == \"adadelta\":\n","  optimizer = optimizers.Adadelta(lr)  # recommended for RNNs\n","  print(\"Used optimizer: adadelta, lr=\"+str(lr))\n","else:\n","  optimizer = optimizers.Adam(lr=lr)\n","  print(\"Used optimizer: Adam, lr=\" + str(lr))\n","\n","# compile model\n","model.compile(optimizer, 'kullback_leibler_divergence', # categorial_crossentropy\n","                           metrics=['accuracy'])\n","if use_class_weights == True:\n","  hist = model.fit([X_train_LSTM, X_train_MLP],\n","                             y_train_one_hot,\n","                             validation_data=([X_test_LSTM, X_test_MLP], y_test_one_hot),\n","                             batch_size=batch_size, epochs=epochs, verbose=1, class_weight=class_weights,\n","                              callbacks=[\n","                               EarlyStoppingOnF1(epochs,\n","                                                 X_test_LSTM, X_test_MLP, y_test,\n","                                                 loss_filename, epsilon=0.0, min_epoch=min_epoch),])\n","else:\n","  hist = model.fit([X_train_LSTM, X_train_MLP],\n","                             y_train_one_hot,\n","                             validation_data=([X_test_LSTM, X_test_MLP], y_test_one_hot),\n","                             batch_size=batch_size, epochs=epochs, verbose=1,\n","                           callbacks=[\n","                               EarlyStoppingOnF1(epochs,\n","                                                 X_test_LSTM, X_test_MLP, y_test,\n","                                                 loss_filename, epsilon=0.0, min_epoch=min_epoch),])\n","  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIzK9drzHIyM"},"outputs":[],"source":["model.save(\"save.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jETm9GZSL25i"},"outputs":[],"source":["model.save(deep_model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WV9kszEgJgH9"},"outputs":[],"source":["print(\"Loading model from:\" + \"save.h5\")\n","model = load_model(\"save.h5\")\n","if (model != None):\n","  X_test_LSTM, X_test_MLP = split_X(X_test, MAX_SEQ_LENGTH)\n","predicted_one_hot = model.predict([X_test_LSTM, X_test_MLP])\n","predicted_int = np.argmax(predicted_one_hot, axis=-1)"]},{"cell_type":"markdown","metadata":{"id":"ittuZaaIrk3i"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-rvnwzcrJ_xc"},"outputs":[],"source":["y_test_str = le.inverse_transform(y_test)\n","y_pred_str = le.inverse_transform(predicted_int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0Lz1i6vJF5-"},"outputs":[],"source":["y_test_str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGhFM0zcJLdq"},"outputs":[],"source":["y_pred_str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChlIVSHMKeRe"},"outputs":[],"source":["score, cm = score_submission(y_test_str, y_pred_str)\n","fold_score, _ = score_submission(y_test_str, y_pred_str)\n","max_fold_score, _ = score_submission(y_test_str, y_test_str)\n","score = fold_score / max_fold_score\n","\n","print(\"FNC-1 score from restored model: \" +  str(score) +\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lH1XMFh7ot7t"},"outputs":[],"source":["fold_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzS039gBovXX"},"outputs":[],"source":["max_fold_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zlb4p0aCKtIy"},"outputs":[],"source":["def print_confusion_matrix(cm):\n","    lines = []\n","    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n","    line_len = len(header)\n","    lines.append(\"-\"*line_len)\n","    lines.append(header)\n","    lines.append(\"-\"*line_len)\n","\n","    hit = 0\n","    total = 0\n","    for i, row in enumerate(cm):\n","        hit += row[i]\n","        total += sum(row)\n","        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n","                                                                   *row))\n","        lines.append(\"-\"*line_len)\n","    print('\\n'.join(lines))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYIs6CZmKyAc"},"outputs":[],"source":["print_confusion_matrix(cm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyxLIS-bke2s"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","print( classification_report(y_test_str, y_pred_str) )\n","\n","# macro avg = unweighted mean of F1 values ***\n","# weighted avg = weighted mean of F1 values based on classes support/number of instances\n","# micro avg = a global F1 value based on sum of TPs, FPs, FNs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NcyoSh94K1mx"},"outputs":[],"source":["# def get_head_body_tuples(include_holdout=False):\n","#     # file paths\n","#     '''\n","#     data_path = \"%s/data/fnc-1\" % (path.dirname(path.dirname(path.dirname(path.abspath(__file__)))))\n","#     splits_dir = \"%s/data/fnc-1/splits\" % (path.dirname(path.dirname(path.dirname(path.abspath(__file__)))))\n","#     dataset = DataSet(data_path)\n","#     '''\n","#     data_path = myConstants.data_path\n","#     splits_dir = myConstants.splits_dir\n","#     dataset = myConstants.d\n","\n","#     def get_stances(dataset, folds, holdout):\n","#         # Creates the list with a dict {'headline': ..., 'body': ..., 'stance': ...} for each\n","#         # stance in the data set (except for holdout)\n","#         stances = []\n","#         for stance in dataset.stances:\n","#             if stance['Body ID'] in holdout and include_holdout == True:\n","#                 stances.append(stance)\n","#             for fold in folds:\n","#                 if stance['Body ID'] in fold:\n","#                     stances.append(stance)\n","\n","#         return stances\n","\n","#     # create new vocabulary\n","#     folds, holdout = kfold_split(data_train, n_folds=10, base_dir=splits_dir)  # [[133,1334,65645,], [32323,...]] => body ids for each fold\n","#     stances = get_stances(dataset, folds, holdout)\n","\n","#     print(\"Stances length: \" + str(len(stances)))\n","\n","#     h = []\n","#     b = []\n","#     # create the final lists with all the headlines and bodies of the set except for holdout\n","#     for stance in stances:\n","#         h.append(stance['Headline'])\n","#         b.append(dataset.articles[stance['Body ID']])\n","\n","#     return h, b\n","\n","\n","\n","\n","# def get_head_body_tuples_test():\n","#     d = myConstants.testdataset\n","\n","#     h = []\n","#     b = []\n","#     for stance in d.stances:\n","#         h.append(stance['Headline'])\n","#         b.append(d.articles[int(stance['Body ID'])])\n","\n","#     return h, b"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}