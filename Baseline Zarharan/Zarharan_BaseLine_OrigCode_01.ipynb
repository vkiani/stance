{"cells":[{"cell_type":"markdown","metadata":{"id":"EAgNOJBVVMQ_"},"source":["# Persian Stance Classification - Deep Learning"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"iMgbaBPU5wuC","executionInfo":{"status":"ok","timestamp":1665212443864,"user_tz":-210,"elapsed":4094,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import sys\n","import datetime\n","import argparse\n","import os\n","import csv\n","import numpy as np\n","import os.path as path"]},{"cell_type":"markdown","metadata":{"id":"VAvZfSsgZoRd"},"source":["# Read Data set\n"]},{"cell_type":"markdown","metadata":{"id":"4pvBMbjyZ7EN"},"source":["**claim and body**\n","\n","```\n","1997\n","\n","748\n","```\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"5eNNmTGnZb4f","outputId":"11920e25-21fa-46b0-e0b7-bca01d2380f0","executionInfo":{"status":"ok","timestamp":1665212443866,"user_tz":-210,"elapsed":33,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfrom pathlib import Path\\nfrom google.colab import drive\\n\\ndrive.mount(\\'/content/drive\\')\\n\\nGDRIVE_DIR = Path(\"/content/drive/My Drive\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["\"\"\"\n","from pathlib import Path\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","GDRIVE_DIR = Path(\"/content/drive/My Drive\")\n","\"\"\""]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y5AuRNZShkuX","executionInfo":{"status":"ok","timestamp":1665212471701,"user_tz":-210,"elapsed":27864,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"outputId":"29035b76-df11-452d-87e1-e4c519ba6f97"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjBiHamk6v4o","outputId":"c6ce44b8-b837-4084-8205-f0406fe289bc","executionInfo":{"status":"ok","timestamp":1665212471704,"user_tz":-210,"elapsed":29,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["number of samples :  1997\n"]}],"source":["import glob\n","\n","# txt to CSV\n","\n","data_path = \"/content/drive/My Drive/Stance Detection Project/ArticleToClaim.txt\"\n","csv_path = \"/content/drive/My Drive/Stance Detection Project/ArticleToClaim.csv\"\n","\n","data_file = glob.glob(data_path)\n","if len(data_file) == 0:\n","  raise Exception('Data file not found at ' + data_path)\n","\n","row_documents = []\n","cnt = 1\n","\n","for file in data_file:\n","    with open (file, \"r\", encoding=\"utf-8\") as fp:\n","        line = fp.readline() #first line is for the headers (Claim, Body Text, Claim Is Question, Claim Has Tow Parts, Stance)\n","        content = fp.read() #read the rest of the file to a string\n","\n","row_documents = content.split(\"#@@@@@#\") #split the instances by the custom delimiter\n","row_documents = list(filter(None, row_documents)) #remove empty instance\n","print(\"number of samples : \", len(row_documents))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"aOoWStpTGqf8","executionInfo":{"status":"ok","timestamp":1665212473389,"user_tz":-210,"elapsed":1701,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"449da8b8-8259-4976-feac-98f8d8f09930"},"outputs":[{"output_type":"stream","name":"stdout","text":["converting text data to csv file..\n","(0, 'مسمومیت زا بودن شدید قارچهای خودرو (وحشی) مشابه قارچهای موجود در بازار !', 'به گزارش اقتصادآنلاین به نقل از تسنیم، رخداد مسمومیت قارچی بهار 97؛ حادثه\\u200cهای مرگباری بود که نه از طریق تصادف یا سقوط و یا غرق شدگی - برای مسافران و گردشگران رخ داده باشد؛ بلکه در رویدادی کم تکرار - علت این مرگ\\u200cها قارچ بوده است! بطوریکه تا  غروب دوشنبه 31 اردیبهشت 1017 نفر به بیمارستان\\u200cها مراجعه کردند که از این تعداد 116 نفر در بیمارستان بستری شدند و متأسفانه 15 نفر جان خود را از دست دادند.\\n\\nدر این زمینه برای واکاوی بیشتر این رویداد تلخ و تاسف بار که می\\u200cطلبید برخی نهادهای مسؤل در حوزه طبیعت از جمله سازمان محیط زیست و .... قبل از وقوع این حوادث که به زعم کارشناسان امر یکی از دلایل مهم آن بارندگی زیاد و رویش این قارچ\\u200cها بود، مردم نسبت به استفاده نکردن خوراکی این قارچ\\u200cها حداقل از نظر آگاهی رسانی مطلع و به آنان هشدار داده می\\u200cشد.', '0', '0', 'Discuss', 'Discuss')\n"]}],"source":["print('converting text data to csv file..')\n","claim = []\n","body = []\n","question = []\n","part = []\n","headline = []\n","label = []    \n","index = []\n","i = 0\n","row_doc = np.asarray(row_documents)\n","for row in row_doc:\n","    claim.append(row.split(',')[0])\n","    body.append(row.split(',')[1])\n","    question.append(row.split(',')[2])\n","    part.append(row.split(',')[3])\n","    headline.append(row.split(',')[4])\n","    label.append(row.split(',')[-1])\n","    index.append(i)\n","    i += 1\n","    \n","Dataset = list(zip(index, claim, body, question, part, headline, label))\n","print(Dataset[0])"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"qZo7W-BrHPQB","executionInfo":{"status":"ok","timestamp":1665212473390,"user_tz":-210,"elapsed":58,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e7262cd9-9075-4749-bb2f-b277acdfd1f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["done!\n"]}],"source":["df = pd.DataFrame(data = Dataset, columns=['index', 'claim', 'body', 'question', 'part', 'headline', 'label'])\n","df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n","\n","print('done!')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"wTYhVbW5ovEh","executionInfo":{"status":"ok","timestamp":1665212473391,"user_tz":-210,"elapsed":51,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["dataset = pd.read_csv('/content/drive/My Drive/Stance Detection Project/ArticleToClaim.csv', index_col = 0)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"25NC72fy3VV5","executionInfo":{"status":"ok","timestamp":1665212473392,"user_tz":-210,"elapsed":51,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["dataset = dataset.sort_index()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"PWGYroc8aFa5","executionInfo":{"status":"ok","timestamp":1665212473393,"user_tz":-210,"elapsed":51,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"03680eb4-3136-4eb8-c1fb-f000b2677cc7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['claim', 'body', 'question', 'part', 'headline', 'label'], dtype='object')"]},"metadata":{},"execution_count":9}],"source":["dataset.columns"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"bLIiViZiabv8","executionInfo":{"status":"ok","timestamp":1665212473394,"user_tz":-210,"elapsed":43,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["claim = dataset['claim']\n","headline = dataset['headline']\n","body = dataset['body']\n","label = dataset['label']"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"IT_sz_7fbYq-","executionInfo":{"status":"ok","timestamp":1665212473395,"user_tz":-210,"elapsed":42,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3b606e06-c73b-452a-f82b-ffd90e998523"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1997"]},"metadata":{},"execution_count":11}],"source":["len(body)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"JV2OY21RatRc","executionInfo":{"status":"ok","timestamp":1665212473396,"user_tz":-210,"elapsed":37,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb3b2a10-dd13-40c3-cdcb-07ccd185247a"},"outputs":[{"output_type":"stream","name":"stdout","text":["مسمومیت زا بودن شدید قارچهای خودرو (وحشی) مشابه قارچهای موجود در بازار ! \n"," به گزارش اقتصادآنلاین به نقل از تسنیم، رخداد مسمومیت قارچی بهار 97؛ حادثه‌های مرگباری بود که نه از طریق تصادف یا سقوط و یا غرق شدگی - برای مسافران و گردشگران رخ داده باشد؛ بلکه در رویدادی کم تکرار - علت این مرگ‌ها قارچ بوده است! بطوریکه تا  غروب دوشنبه 31 اردیبهشت 1017 نفر به بیمارستان‌ها مراجعه کردند که از این تعداد 116 نفر در بیمارستان بستری شدند و متأسفانه 15 نفر جان خود را از دست دادند.\n","\n","در این زمینه برای واکاوی بیشتر این رویداد تلخ و تاسف بار که می‌طلبید برخی نهادهای مسؤل در حوزه طبیعت از جمله سازمان محیط زیست و .... قبل از وقوع این حوادث که به زعم کارشناسان امر یکی از دلایل مهم آن بارندگی زیاد و رویش این قارچ‌ها بود، مردم نسبت به استفاده نکردن خوراکی این قارچ‌ها حداقل از نظر آگاهی رسانی مطلع و به آنان هشدار داده می‌شد. \n"," Discuss\n"]}],"source":["print(claim[0], \"\\n\", body[0], \"\\n\", label[0])"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"P8S0PS0VzZ1X","executionInfo":{"status":"ok","timestamp":1665212473397,"user_tz":-210,"elapsed":35,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/","height":264},"outputId":"aa1fecde-8ebd-4dc2-eef1-772f53215222"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 288x216 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQ0AAAD3CAYAAAAHbAHDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATt0lEQVR4nO3de7CdVX3G8e9DuETAQgIxExE9UVMU0UqIGIxaMUK5KLY2IEgNZTJDL/GCdKqxtcXxClaLSNtMkQBS44WirS0gGGOQSguaICYgIBkCCgYSIEQQMQn59Y937bCzOclhnXefvfbl+cycOWev/e6zf3sgz3kv610/RQRmZs/WLqULMLPe4tAwsywODTPL4tAwsywODTPL4tAwsyy7li5gZ/bff/8YGhoqXYbZQFqxYsVDETGpdbyrQ2NoaIjly5eXLsNsIEm6d7hxH56YWRaHhpllcWiYWRaHhpllcWiYWZauvnpig2lowVUdfb97zjm+o+/X67ynYWZZHBpmlsWhYWZZHBpmlsWhYWZZHBpmlmXE0JB0saR1km5tGpsoaYmku9L3CWlckr4gabWklZKmN73mtLT9XZJOG5uPY2Zj7dnsaVwKHNMytgBYGhHTgKXpMcCxwLT0dQawEKqQAc4GXgscDpzdCBoz6y0jhkZEXA880jL8duBL6ecvAX/YNH5ZVG4E9pU0BfgDYElEPBIRG4AlPDOIzKwHjPacxuSIWJt+fgCYnH4+APhF03b3pbEdjT+DpDMkLZe0fP369aMsz8zGSu0ToVF1W2pbx6WIuDAiZkTEjEmTnrFokJkVNtrQeDAddpC+r0vj9wMHNm33gjS2o3Ez6zGjDY3/AhpXQE4DvtU0PjddRZkJbEyHMdcCR0uakE6AHp3GzKzHjHiXq6SvAm8C9pd0H9VVkHOAyyXNA+4FTkqbXw0cB6wGngBOB4iIRyR9HPhR2u5jEdF6ctXMesCIoRERp+zgqdnDbBvA/B38nouBi7OqM7Ou4xmhZpbFoWFmWRwaZpbFoWFmWRwaZpbFoWFmWRwaZpbFoWFmWRwaZpbFoWFmWRwaZpbFoWFmWRwaZpbFoWFmWRwaZpbFoWFmWRwaZpbFoWFmWRwaZpbFoWFmWWqFhqQPSLpN0q2SvippvKSpkm5KTaC/Lmn3tO0e6fHq9PxQOz6AmXXWqEND0gHA+4AZEXEIMA44GTgXOC8iXgpsAOall8wDNqTx89J2ZtZj6h6e7Ao8R9KuwJ7AWuDNwBXp+dbm0I2m0VcAsyWp5vubWYeNOjQi4n7gs8DPqcJiI7ACeDQitqTNmhs9b2sCnZ7fCOzX+nvdANqsu9U5PJlAtfcwFXg+sBdwTN2C3ADarLvVOTx5C7AmItZHxGbgm8AsYN90uALbN3re1gQ6Pb8P8HCN9zezAuqExs+BmZL2TOcmZgM/BZYBc9I2rc2hG02j5wDfS20czayH1DmncRPVCc2bgVXpd10IfAg4S9JqqnMWi9JLFgH7pfGzgAU16jazQkZsAL0zEXE2VRf5ZncDhw+z7ZPAiXXez8zK84xQM8vi0DCzLA4NM8vi0DCzLA4NM8vi0DCzLA4NM8vi0DCzLA4NM8vi0DCzLA4NM8vi0DCzLA4NM8vi0DCzLA4NM8vi0DCzLA4NM8vi0DCzLA4NM8vi0DCzLHUbQO8r6QpJd0i6XdIRkiZKWiLprvR9QtpWkr6QGkCvlDS9PR/BzDqp7p7G+cA1EfEy4PeA26laEyyNiGnAUp5uVXAsMC19nQEsrPneZlZAnbaM+wBvJPU1iYhNEfEo2zd6bm0AfVlUbqTqxDZl1JWbWRF19jSmAuuBSyT9WNJFkvYCJkfE2rTNA8Dk9PO2BtBJc3PobdwA2qy71QmNXYHpwMKIOBT4NS1d01LbxazWi24Abdbd6nRYuw+4L7VnhKpF4wLgQUlTImJtOvxYl57f1gA6aW4ObTYwhhZc1dH3u+ec49v6++r0cn0A+IWkg9JQowF0c6Pn1gbQc9NVlJnAxqbDGDPrEbV6uQLvBRZL2p2qh+vpVEF0uaR5wL3ASWnbq4HjgNXAE2lbM+sxdRtA3wLMGOap2cNsG8D8Ou9nZuV5RqiZZXFomFkWh4aZZXFomFkWh4aZZXFomFkWh4aZZXFomFkWh4aZZXFomFkWh4aZZXFomFkWh4aZZXFomFkWh4aZZXFomFkWh4aZZXFomFkWh4aZZXFomFmW2qEhaVzqsHZlejxV0k2p0fPX00rlSNojPV6dnh+q+95m1nnt2NN4P1Xj54ZzgfMi4qXABmBeGp8HbEjj56XtzKzH1AoNSS8AjgcuSo8FvJmq2xo8swF0ozH0FcDstL2Z9ZC6exqfBz4IbE2P9wMejYgt6XFzk+dtDaDT8xvT9mbWQ0YdGpLeCqyLiBVtrMdd4826XJ09jVnACZLuAb5GdVhyPrCvpEbntuYmz9saQKfn9wEebv2l7hpv1t3qNID+cES8ICKGgJOB70XEqcAyYE7arLUBdKMx9Jy0fYz2/c2sjLGYp/Eh4CxJq6nOWSxK44uA/dL4WcCCMXhvMxtjdbvGAxAR1wHXpZ/vBg4fZpsngRPb8X5mVo5nhJpZFoeGmWVxaJhZFoeGmWVxaJhZFoeGmWVxaJhZFoeGmWVxaJhZFoeGmWVxaJhZFoeGmWVxaJhZFoeGmWVxaJhZFoeGmWVxaJhZFoeGmWVxaJhZFoeGmWWp0yzpQEnLJP1U0m2S3p/GJ0paIumu9H1CGpekL6QG0CslTW/XhzCzzqmzp7EF+KuIOBiYCcyXdDBVa4KlETENWMrTrQqOBaalrzOAhTXe28wKqdMsaW1E3Jx+foyqc/wBbN/oubUB9GVRuZGqE9uUUVduZkW0pe+JpCHgUOAmYHJErE1PPQBMTj9vawCdNJpDr8WyDC24qqPvd885x3f0/ay71T4RKmlv4BvAmRHxq+bnUtvFrNaLbgBt1t1qhYak3agCY3FEfDMNP9g47Ejf16XxbQ2gk+bm0Nu4AbRZd6tz9URU/Vlvj4h/bHqqudFzawPouekqykxgY9NhjJn1iDrnNGYB7wZWSboljf0NcA5wuaR5wL3ASem5q4HjgNXAE8DpNd7bzAoZdWhExA8A7eDp2cNsH8D80b6fmXUHzwg1sywODTPL4tAwsywODTPL0pYZod3GMybNxo73NMwsi0PDzLI4NMwsi0PDzLI4NMwsi0PDzLI4NMwsi0PDzLI4NMwsi0PDzLI4NMwsi0PDzLI4NMwsi0PDzLI4NMwsS8dDQ9Ixku5MjaAXjPwKM+smHQ0NSeOAf6ZqBn0wcEpqGm1mPaLTexqHA6sj4u6I2AR8jaoxtJn1iE6Hxo6aQJtZj+i6NUIlnQGckR4+LunODr79/sBDuS/SuWNQydjw5xuGP98OvWi4wU6HxohNoCPiQuDCThbVIGl5RMwo8d6d4M/X27rl83X68ORHwDRJUyXtDpxM1RjazHpER/c0ImKLpPcA1wLjgIsj4rZO1mBm9XT8nEZEXE3VQb4bFTks6iB/vt7WFZ9PVTN3M7Nnx9PIzSyLQ8PMsjg0zCxL103uMht0kqbv7PmIuLlTtQxnoE+ESpoMfAp4fkQcm26eOyIiFhUurW0k/S6wEJgcEYdIehVwQkR8onBpbSHpROCaiHhM0keA6cAnSv/DqkPSsvTjeGAG8BNAwKuA5RFxRKnawIcnl1LNGXl+evwz4Mxi1YyNLwIfBjYDRMRKqkl1/eLvUmC8HngLsIgqJHtWRBwZEUcCa4HpETEjIg4DDqVlBnUJgx4a+0fE5cBWqCafAU+VLant9oyIH7aMbSlSydho/Pc6HrgwIq4Cdi9YTzsdFBGrGg8i4lbg5QXrAXxO49eS9gMCQNJMYGPZktruIUkv4enPOIfqL1i/uF/SvwJHAedK2oP++WO4UtJFwJfT41OBlQXrAXxOYzpwAXAIcCswCZiTduH7gqQXU80kfB2wAVgD/ElE3FOyrnaRtCdwDLAqIu6SNAV4ZUR8p3BptUkaD/wF8MY0dD2wMCKeLFfVgIcGgKRdgYOoTjTdGRGbC5c0JiTtBewSEY+VrqWd0l7UfRHxW0lvojpZeFlEPFq2svaQ9BzghRHRySUidqpfduNGJf2VWgCcmY4XhyS9tXBZbSVpsqRFwBXphOHBkuaVrquNvgE8JemlVHtUBwJfKVtSe0g6AbgFuCY9frWk4neFD3RoAJcAm4DGJaz7gb64FNnkUvr7CtHWdAL7HcAFEfHXwJTCNbXL2VRLZD4KEBG3AFOLVoRD4yUR8Rmevhz5BNVhSj/p9ytEmyWdAswFrkxjuxWsp502R0Trifni5xMGPTQ2pWPGxpWFlwC/LVtS2/X7FaLTqfYUPxkRayRNBf6tcE3tcpukdwHjJE2TdAHwv6WLGugToZKOAj5C1U7hO8As4E8j4rqSdbXTIFwh6lfpnNvfAkenoWuBj0dE0T9sAxsaknYB5gBLgZlUhyU3RkT2wq3dKvWZeR9VaPTlFSJJaxhmlz0iXlygnLaSdGJE/PtIY502sKEB3bNQ61iS9MOIOLx0HWMlHXo1jAdOBCZGxN8XKqltJN0cEdNHGuu0QQ+Nc6iWhP868OvGeEQ8UqyoNpN0HtWJwdbP2LM3dI1E0op0r0ZPknQscBxwEtV/t4bfAQ4u/Udg0KeRvzN9n980FkDP79o2eXX6/rGmsQDeXKCWtmu5jXwXqrtCe/3/618Cy4ETgBVN448BHyhSUZOB3tOw3td0GzlUN+KtAT7XTTMoR0vSbt14/mmgQ0PSO4YZ3kh1H8O6TtczFiSdNczwRmBFmixkXUrSNODTVFf3xjfGS5/kHfR5GvOAi6juHjyVau2JDwE3SHp3ycLaaAbw51Q9cw8A/ozqBq8vSvpgycLaQdKnJO3b9HiCpH6Z1XsJ1dogW4Ajgct4+o7XYgZ9T+NaYG5EPJgeT6b6D3MKcH1EHFKyvnaQdD1wXEQ8nh7vDVxFFRwrIuLgkvXVJenHEXFoy1jxKwzt0DihK2lVRLyyeaxkXb1+wqiuAxuBkaxLY49I6rpjyVF6HtvPct1MtfTfbyT1w+zXcZL2aEx4SjN89yhcU7v8Ns0nuit1Jrwf2LtwTQMfGtdJuhJoTJb54zS2F+kmoT6wGLhJ0rfS47cBX0mf8aflymqbxcBSSZekx6cDXypYTzu9H9iTaoLex6mueJ1WtCJ8eCKquyNfn4Y2UP0Vnr/jV/UeSa+hWoQH4IaIWF6ynnaTdAzV+qAASyLi2pL19LuBDg0ASYcC76KaSbgG+EZE/FPZqtpP0vPY/gz8zwuW0zZpj+k3EbFV0kFU0+W/3Y2XKp8tSf/NTu5mjYgTOljOMwzk4Ula1v+U9NWYEaq0AnRfSQu5fI5qPY11wAuBO4BXlKyrja4H3iBpAtViNcupJu2dWrSqej5buoCdGcg9DUlbgf8B5kXE6jR2d+nr32NB0k+ojoW/GxGHSjqSao3Qvli9q3GlRNJ7gedExGck3RIRrx7xxT3Ay/11j3dQrci9TNIXJc2m/xbfadgcEQ8Du0jaJSKWUc3d6BeSdATVnsVVaWxcwXraRtLb8HJ/3SEi/jMiTgZeBiyjWv7ueZIWSjp656/uOY+muRnXA4slnU/TjWt94EyqZlD/ERG3pdXXl43wml7xUbpwub+BPDwZTjomPhF4Z0TMLl1PuzROFFL9gTgV2AdYnPY+rItJujEiZjZPYJO0MiJeVbKugTwROpyI2EC1mvWFpWsZCxGxRdL/Ue1d/ap0PXVJ+nxEnLmjKw2lrzC0yXbL/VHN1/Byfza2JK0A3gBMAG4AfgRsiohevrqApMMiYoWk3x/u+Yj4fqdrarcdLPf3CTdLsjHV71cXACRNAoiI9aVraZe0VON3u3EawECeCB0w/Xx14aOSHgLuBH4mab2knl/mDyAingK2StqndC2tfE6j//Xl1YW0Tsgs4DURsSaNvRhYKOkDEXFe0QLb43FglaQlbL9U4/vKleTDE+tRkn4MHNW6enw6VPlO6+3yvUjSsDenRUTRG/K8p9GnBuDqwm7DtZuIiPWS+qLDWulw2BGHRv9qdBnr6vsYatg0yud6hqRZVBO8XkT1b1VAlL7dwYcnA6BPry48xfAzWwWMj4ie39uQdAfV6uMraOq/W3pinvc0+pikjwLvobpKJklbqDqrf2ynL+wBEdEXV4BGsDEivl26iFa+5NqnWq4uTIyICcBrgVmSivfOsGdlmaR/kHSEpOmNr9JF+fCkTw3C1YV+19TTpfGPtHFOo2ijKx+e9K++v7rQr5p61VyZvgewHvhBY05KST486V99f3Whjz03fe2dvp5LtQbKtyWdXLIw8OFJ3xqEqwuDRtJEqvtRip7X8OFJnxqQqwsDJfXjKb7CnA9PzHpEWt91Q+k6vKdh1mUkreKZU/8nAr8E5na+ou35nIZZl5H0opahAB6OiK5Y29WhYWZZfE7DzLI4NMwsi0PDskh6fITnhyTdmvk7L5U0p15l1ikODTPL4tCwUZG0t6Slkm6WtErS25ue3lXSYkm3S7oiLcWPpMMkfV/SCknXSppSqHyrwaFho/Uk8EdpSvORwOeaZiseBPxLRLycqjHTX6ab5C4A5kTEYcDFwCcL1G01eXKXjZaAT0l6I7AVOACYnJ77RUTckH7+MlVnsGuAQ4AlKVvGUTXhth7j0LDROhWYBBwWEZsl3QOMT8+1Tv4JqpC5LSKO6FyJNhZ8eGKjtQ+wLgXGkVSL3za8MDVoAngX8AOqhkaTGuOSdpP0io5WbG3h0LDRWgzMSPdJzAXuaHruTmC+pNupesgujIhNwBzgXEk/AW4BXtfhmq0NPI3czLJ4T8PMsjg0zCyLQ8PMsjg0zCyLQ8PMsjg0zCyLQ8PMsjg0zCzL/wPmbmmPoVE34wAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}],"source":["import matplotlib.pyplot as plt\n","\n","fig = plt.figure(figsize=(4,3))\n","dataset.groupby('label').headline.count().plot.bar(ylim=0)\n","plt.show()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"jo637O3-zaMi","executionInfo":{"status":"ok","timestamp":1665212473398,"user_tz":-210,"elapsed":35,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"17313324-7c1f-481d-e0a0-15c3f7ed11f7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["label\n","Agree         137\n","Disagree      206\n","Discuss      1068\n","Unrelated     586\n","Name: headline, dtype: int64"]},"metadata":{},"execution_count":14}],"source":["dataset.groupby('label').headline.count()"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"FVFvfaRAnGeK","executionInfo":{"status":"ok","timestamp":1665212473399,"user_tz":-210,"elapsed":31,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3cbc707f-8d14-44a1-ec59-009ee8f63484"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1997"]},"metadata":{},"execution_count":15}],"source":["len(dataset)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"c8ZJraSam5rG","executionInfo":{"status":"ok","timestamp":1665212473400,"user_tz":-210,"elapsed":26,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"aa32fb0d-7020-4a72-d15d-6df5be47780c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["label\n","Agree         6.860290\n","Disagree     10.315473\n","Discuss      53.480220\n","Unrelated    29.344016\n","Name: headline, dtype: float64"]},"metadata":{},"execution_count":16}],"source":["dataset.groupby('label').headline.count() * 100/len(dataset)"]},{"cell_type":"markdown","metadata":{"id":"ibRE0Lj9tSMa"},"source":["# Clean Data"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"pLuPx6X7tV8X","executionInfo":{"status":"ok","timestamp":1665212478243,"user_tz":-210,"elapsed":4865,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"66066a76-7279-4628-cb9f-cecf5dcecf1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting stanfordnlp\n","  Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n","\u001b[K     |████████████████████████████████| 158 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (3.17.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (2.23.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (1.12.1+cu113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->stanfordnlp) (4.1.1)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanfordnlp) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (2.10)\n","Installing collected packages: stanfordnlp\n","Successfully installed stanfordnlp-0.2.0\n"]}],"source":["!pip install stanfordnlp"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"qUA9OlQctvSR","executionInfo":{"status":"ok","timestamp":1665212633644,"user_tz":-210,"elapsed":155419,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"62700a5f-5624-4cfb-a52c-acc8144f5a53"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using the default treebank \"fa_seraji\" for language \"fa\".\n","Would you like to download the models for: fa_seraji now? (Y/n)\n","y\n","\n","Default download directory: /root/stanfordnlp_resources\n","Hit enter to continue or type an alternate directory.\n","/root/stanfordnlp_resources\n","\n","Downloading models for: fa_seraji\n","Download location: /root/stanfordnlp_resources/fa_seraji_models.zip\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 226M/226M [00:38<00:00, 5.83MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Download complete.  Models saved to: /root/stanfordnlp_resources/fa_seraji_models.zip\n","Extracting models file for: fa_seraji\n","Cleaning up...Done.\n"]}],"source":["import stanfordnlp\n","stanfordnlp.download('fa')"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"ygMCI-pft5qW","executionInfo":{"status":"ok","timestamp":1665212633645,"user_tz":-210,"elapsed":22,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a5096065-3759-4039-a1c2-2bd51f5bae67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Use device: cpu\n","---\n","Loading: tokenize\n","With settings: \n","{'model_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji_tokenizer.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n","---\n","Loading: lemma\n","With settings: \n","{'model_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji_lemmatizer.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n","Building an attentional Seq2Seq model...\n","Using a Bi-LSTM encoder\n","Using soft attention for LSTM.\n","Finetune all embeddings.\n","[Running seq2seq lemmatizer with edit classifier]\n","Done loading processors!\n","---\n"]}],"source":["import stanfordnlp\n","nlp = stanfordnlp.Pipeline(processors='tokenize,lemma', lang='fa', treebank=None, use_gpu=True) "]},{"cell_type":"code","execution_count":20,"metadata":{"id":"jMU7fE5c1bIh","executionInfo":{"status":"ok","timestamp":1665212649374,"user_tz":-210,"elapsed":15745,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f11d773-fd92-49e5-ef2b-0c15e73e4dfa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting hazm\n","  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n","\u001b[K     |████████████████████████████████| 316 kB 5.3 MB/s \n","\u001b[?25hCollecting nltk==3.3\n","  Downloading nltk-3.3.0.zip (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 52.0 MB/s \n","\u001b[?25hCollecting libwapiti>=0.2.1\n","  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n","\u001b[K     |████████████████████████████████| 233 kB 68.6 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n","Building wheels for collected packages: nltk, libwapiti\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394486 sha256=21be762277b14f8f3a52190b6fd5f42a731aaaad2cf8126accec14adb8a6aee9\n","  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n","  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154586 sha256=0b51215713addce263ca38be4ac6ad989db0e0af8e1351efc140104e2d9f2283\n","  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n","Successfully built nltk libwapiti\n","Installing collected packages: nltk, libwapiti, hazm\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.7\n","    Uninstalling nltk-3.7:\n","      Successfully uninstalled nltk-3.7\n","Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"]}],"source":["!pip install hazm"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"Zdy5FjpMvUqZ","executionInfo":{"status":"ok","timestamp":1665212651215,"user_tz":-210,"elapsed":1849,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["from __future__ import unicode_literals\n","from hazm import *\n","\n","  \n","k = []\n","with open(\"/content/drive/My Drive/Stance Detection Project/StopWords_fa.txt\", 'r', encoding=\"utf-8\") as f:\n","    for word in f:\n","        word = word.split('\\n')\n","        k.append(word[0])\n","        \n","def remove_stopwords(text):\n","    sw_data = []\n","    for i in text:\n","        for j in k:\n","            if j in word_tokenize(i):\n","                i.replace(j, '')\n","        sw_data.append(i)\n","    return sw_data\n","\n","\n","def remove_slash(text):\n","    ext_data = []\n","    for i in text:\n","        if '/' in i:\n","            spl = i.split('/')\n","            if 'شایعه' in spl[-1]:\n","                i = i.replace(spl[-1], '')\n","        ext_data.append(i)\n","    return ext_data\n","\n","\n","import re\n","r = re.compile(\"[\\!\\;,؟:?،؛.+»«<>|\\#(\\)\\-\\/\\'\\\"]\")\n","def remove_punc(text):\n","    punc = []\n","    for i in text:\n","        punc.append(r.sub(\"\", i))\n","    return punc\n","\n","extra_str = ['\\u200c', '\\u200d', '\\u200e', '\\u200b', '\\r', '\\n', '\\ufeff']\n","def clean_data(text):\n","    \n","    print(\"start cleaning data..\")\n","    \n","    text = remove_slash(text)\n","    \n","    clean_data = []\n","    for i in text:\n","        for j in extra_str:\n","            if j in i:\n","                i = i.replace(j,'')\n","        clean_data.append(i)\n","    \n","    print(\"data is ready!\")\n","    return clean_data"]},{"cell_type":"code","source":[],"metadata":{"id":"DCGBH_OQiyWx","executionInfo":{"status":"ok","timestamp":1665212651215,"user_tz":-210,"elapsed":18,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":22,"metadata":{"id":"kZk9XZU7Q8U3","executionInfo":{"status":"ok","timestamp":1665212651217,"user_tz":-210,"elapsed":19,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["import re\n","r = re.compile(\"[\\!\\;,؟:?،؛.+»«<>|\\#(\\)\\-\\/\\'\\\"]\")\n","def clean(text):\n","    return r.sub(\"\", text)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"EOeLIjB_wfa1","executionInfo":{"status":"ok","timestamp":1665212651218,"user_tz":-210,"elapsed":20,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bc33a2c4-bdea-4e2b-8786-aa8d59eeefce"},"outputs":[{"output_type":"stream","name":"stdout","text":["start cleaning data..\n","data is ready!\n"]}],"source":["clean_claim = clean_data(claim)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"RKhhrM922IbE","executionInfo":{"status":"ok","timestamp":1665212651219,"user_tz":-210,"elapsed":20,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f883c9dd-03d6-4e6e-e0d5-b22652f1d161"},"outputs":[{"output_type":"stream","name":"stdout","text":["start cleaning data..\n","data is ready!\n"]}],"source":["clean_body = clean_data(body)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"M69ZHTh538Sb","executionInfo":{"status":"ok","timestamp":1665212651220,"user_tz":-210,"elapsed":18,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6a275f9-7076-4f1d-b888-fa01b3d085b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["save clean data.\n"]}],"source":["Dataset = list(zip(clean_claim, clean_body, label))\n","np.random.shuffle(Dataset)  \n","df = pd.DataFrame(data = Dataset, columns=['claim', 'body', 'label'])\n","df.to_csv('clean_claim_body.csv', index=True, encoding=\"utf-8\")\n","\n","print('save clean data.')"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"fa_zU2y2vD_K","executionInfo":{"status":"ok","timestamp":1665212651833,"user_tz":-210,"elapsed":628,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["import pandas as pd\n","dataset_clean = pd.read_csv('clean_claim_body.csv', index_col = 0, )"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"PUhHHXho1a9n","executionInfo":{"status":"ok","timestamp":1665212651839,"user_tz":-210,"elapsed":70,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["clean_claim = dataset_clean['claim']\n","clean_body = dataset_clean['body']"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"rJUbSCMfvUqC","executionInfo":{"status":"ok","timestamp":1665212651841,"user_tz":-210,"elapsed":70,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"3a5dacc3-33a1-4fde-fef2-37a1141b748d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               claim  \\\n","0              مصدومیت مهدی هاشمی در صحنه فیلمبرداری   \n","1  مقاله اهانت آمیز نشریه صبح صادق در مورد چاه جم...   \n","2  تجاوز به دختر بازداشت شده و واژگونی ون گشت ارش...   \n","3       جعلی بودن خالکوبی فمینیستی ترانه علیدوستی! /   \n","4   اعلام عمدی بودن آتش سوزی ساختمان شرکت برق حرا...   \n","\n","                                                body      label  \n","0  به تازگی در شب گذشته خبری در رسانه ها و فضای م...    Discuss  \n","1  به گزارش خبرنگار دفاعی امنیتی دفاع پرس؛ «هشدار...    Discuss  \n","2  به گزارش افکارنیوز، رئیس پلیس سایبری کشور ضمن ...    Discuss  \n","3  به گزارش سیتنا، در میان تصاویری که از نشست خبر...    Discuss  \n","4  بهگزارش برق نیوز، در جلسه عصر امروز شورای معاو...  Unrelated  "],"text/html":["\n","  <div id=\"df-7b3da429-242c-4f8b-8b3e-caf6939071c6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>claim</th>\n","      <th>body</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>مصدومیت مهدی هاشمی در صحنه فیلمبرداری</td>\n","      <td>به تازگی در شب گذشته خبری در رسانه ها و فضای م...</td>\n","      <td>Discuss</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>مقاله اهانت آمیز نشریه صبح صادق در مورد چاه جم...</td>\n","      <td>به گزارش خبرنگار دفاعی امنیتی دفاع پرس؛ «هشدار...</td>\n","      <td>Discuss</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>تجاوز به دختر بازداشت شده و واژگونی ون گشت ارش...</td>\n","      <td>به گزارش افکارنیوز، رئیس پلیس سایبری کشور ضمن ...</td>\n","      <td>Discuss</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>جعلی بودن خالکوبی فمینیستی ترانه علیدوستی! /</td>\n","      <td>به گزارش سیتنا، در میان تصاویری که از نشست خبر...</td>\n","      <td>Discuss</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>اعلام عمدی بودن آتش سوزی ساختمان شرکت برق حرا...</td>\n","      <td>بهگزارش برق نیوز، در جلسه عصر امروز شورای معاو...</td>\n","      <td>Unrelated</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b3da429-242c-4f8b-8b3e-caf6939071c6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7b3da429-242c-4f8b-8b3e-caf6939071c6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7b3da429-242c-4f8b-8b3e-caf6939071c6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":28}],"source":["dataset_clean.head()"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"fL2Q4zNQvpT5","executionInfo":{"status":"ok","timestamp":1665212651846,"user_tz":-210,"elapsed":70,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","y = dataset_clean.label\n","X = dataset_clean.drop('label', axis=1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"t_ur1L11xOJL","executionInfo":{"status":"ok","timestamp":1665212651848,"user_tz":-210,"elapsed":71,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f4c24186-c1aa-415d-cce0-d58ab033b3f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["save train data.\n"]}],"source":["Data_train = list(zip(X_train['claim'], X_train['body'], y_train))\n","    \n","df = pd.DataFrame(data = Data_train, columns=['claim', 'body', 'label'])\n","df.to_csv('train_data.csv', index=True, encoding=\"utf-8\")\n","\n","print('save train data.')"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"atY7I1cxy_6C","executionInfo":{"status":"ok","timestamp":1665212651852,"user_tz":-210,"elapsed":69,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"225dc763-7d3c-40c6-fe5c-efb55d2d5097"},"outputs":[{"output_type":"stream","name":"stdout","text":["save test data.\n"]}],"source":["Data_test = list(zip(X_test['claim'], X_test['body'], y_test))\n","    \n","df = pd.DataFrame(data = Data_test, columns=['claim', 'body', 'label'])\n","df.to_csv('test_data.csv', index=True, encoding=\"utf-8\")\n","\n","print('save test data.')"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"PvvkjOz0ynTI","executionInfo":{"status":"ok","timestamp":1665212651854,"user_tz":-210,"elapsed":64,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["data_train = pd.read_csv('train_data.csv', index_col = 0, )\n","data_test = pd.read_csv('test_data.csv', index_col = 0, )"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"ThVeCTtnysw-","executionInfo":{"status":"ok","timestamp":1665212651855,"user_tz":-210,"elapsed":64,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2f978f30-8591-4557-b8a9-9eee3d005e78"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1597, 400)"]},"metadata":{},"execution_count":33}],"source":["len(data_train), len(data_test)"]},{"cell_type":"markdown","metadata":{"id":"u5kmac9sZtlM"},"source":["# Extract Feature"]},{"cell_type":"markdown","metadata":{"id":"lVM3qiwYiytW"},"source":["## bow"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"SBbHZ8W6bND6","executionInfo":{"status":"ok","timestamp":1665212651856,"user_tz":-210,"elapsed":57,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def ngrams(input, n):\n","    input = input.split(' ')\n","    output = []\n","    for i in range(len(input) - n + 1):\n","        output.append(input[i:i + n])\n","    return output\n","\n","\n","def chargrams(input, n):\n","    output = []\n","    for i in range(len(input) - n + 1):\n","        output.append(input[i:i + n])\n","    return output\n","\n","\n","def append_chargrams(features, text_headline, text_body, size):\n","    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(text_headline.split())), size)]\n","    grams_hits = 0\n","    grams_early_hits = 0\n","    grams_first_hits = 0\n","    for gram in grams:\n","        if gram in text_body:\n","            grams_hits += 1\n","        if gram in text_body[:255]:\n","            grams_early_hits += 1\n","        if gram in text_body[:100]:\n","            grams_first_hits += 1\n","    features.append(grams_hits)\n","    features.append(grams_early_hits)\n","    features.append(grams_first_hits)\n","    return features\n","\n","\n","def append_ngrams(features, text_headline, text_body, size):\n","    grams = [' '.join(x) for x in ngrams(text_headline, size)]\n","    grams_hits = 0\n","    grams_early_hits = 0\n","    for gram in grams:\n","        if gram in text_body:\n","            grams_hits += 1\n","        if gram in text_body[:255]:\n","            grams_early_hits += 1\n","    features.append(grams_hits)\n","    features.append(grams_early_hits)\n","    return features\n","\n","\n","def hand_features(headlines, bodies):\n","\n","    def binary_co_occurence(headline, body):\n","        # Count how many times a token in the title\n","        # appears in the body text.\n","        bin_count = 0\n","        bin_count_early = 0\n","        for headline_token in clean(headline).split(\" \"):\n","            if headline_token in clean(body):\n","                bin_count += 1\n","            if headline_token in clean(body)[:255]:\n","                bin_count_early += 1\n","        return [bin_count, bin_count_early]\n","\n","    def binary_co_occurence_stops(headline, body):\n","        # Count how many times a token in the title\n","        # appears in the body text. Stopwords in the title\n","        # are ignored.\n","        bin_count = 0\n","        bin_count_early = 0\n","        for headline_token in remove_stopwords(clean(headline).split(\" \")):\n","            if headline_token in clean(body):\n","                bin_count += 1\n","                bin_count_early += 1\n","        return [bin_count, bin_count_early]\n","\n","    def count_grams(headline, body):\n","        # Count how many times an n-gram of the title\n","        # appears in the entire body, and intro paragraph\n","\n","        clean_body = clean(body)\n","        clean_headline = clean(headline)\n","        features = []\n","        features = append_chargrams(features, clean_headline, clean_body, 2)\n","        features = append_chargrams(features, clean_headline, clean_body, 8)\n","        features = append_chargrams(features, clean_headline, clean_body, 4)\n","        features = append_chargrams(features, clean_headline, clean_body, 16)\n","        features = append_ngrams(features, clean_headline, clean_body, 2)\n","        features = append_ngrams(features, clean_headline, clean_body, 3)\n","        features = append_ngrams(features, clean_headline, clean_body, 4)\n","        features = append_ngrams(features, clean_headline, clean_body, 5)\n","        features = append_ngrams(features, clean_headline, clean_body, 6)\n","        return features\n","\n","    X = []\n","    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n","        X.append(binary_co_occurence(headline, body)\n","                 + binary_co_occurence_stops(headline, body)\n","                 + count_grams(headline, body))\n","    return X"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"_ZK6HZjTUMdM","executionInfo":{"status":"ok","timestamp":1665212651857,"user_tz":-210,"elapsed":57,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["# x = hand_features(clean_claim, clean_body)"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"UetOVrqMzoR8","executionInfo":{"status":"ok","timestamp":1665212651858,"user_tz":-210,"elapsed":57,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def get_head_body_tuples(include_holdout=False):\n","    # file paths\n","    '''\n","    data_path = \"%s/data/fnc-1\" % (path.dirname(path.dirname(path.dirname(path.abspath(__file__)))))\n","    splits_dir = \"%s/data/fnc-1/splits\" % (path.dirname(path.dirname(path.dirname(path.abspath(__file__)))))\n","    dataset = DataSet(data_path)\n","    '''\n","    data_path = myConstants.data_path\n","    splits_dir = myConstants.splits_dir\n","    dataset = myConstants.d\n","\n","    def get_stances(dataset, folds, holdout):\n","        # Creates the list with a dict {'headline': ..., 'body': ..., 'stance': ...} for each\n","        # stance in the data set (except for holdout)\n","        stances = []\n","        for stance in dataset.stances:\n","            if stance['Body ID'] in holdout and include_holdout == True:\n","                stances.append(stance)\n","            for fold in folds:\n","                if stance['Body ID'] in fold:\n","                    stances.append(stance)\n","\n","        return stances\n","\n","    # create new vocabulary\n","    folds, holdout = kfold_split(data_train, n_folds=10, base_dir=splits_dir)  # [[133,1334,65645,], [32323,...]] => body ids for each fold\n","    stances = get_stances(dataset, folds, holdout)\n","\n","    print(\"Stances length: \" + str(len(stances)))\n","\n","    h = []\n","    b = []\n","    # create the final lists with all the headlines and bodies of the set except for holdout\n","    for stance in stances:\n","        h.append(stance['Headline'])\n","        b.append(dataset.articles[stance['Body ID']])\n","\n","    return h, b"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"AXmoLmIlb6Z6","executionInfo":{"status":"ok","timestamp":1665212651861,"user_tz":-210,"elapsed":59,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def get_head_body_tuples_test():\n","    d = myConstants.testdataset\n","\n","    h = []\n","    b = []\n","    for stance in d.stances:\n","        h.append(stance['Headline'])\n","        b.append(d.articles[int(stance['Body ID'])])\n","\n","    return h, b\n","\n","# this function takes a very long time to finish execution!\n","def negated_context_word_12grams_concat_tf5000_l2_all_data(headlines, bodies):\n","    \"\"\"\n","    Negates string after special negation word by adding a \"NEG_\" in front\n","    of every negated word, until a punctuation mark appears.\n","    Source:\n","        NRC-Canada: Buidling the State-of-the-Art in Sentiment Analysis of Tweets\n","        http://sentiment.christopherpotts.net/lingstruc.html\n","        http://stackoverflow.com/questions/23384351/how-to-add-tags-to-negated-words-in-strings-that-follow-not-no-and-never\n","\n","\n","    :param headlines:\n","    :param bodies:\n","    :return:\n","    \"\"\"\n","    def get_negated_text(text):\n","      sens = text.replace(';','.').replace(',','.').replace('!','.').replace(':','.').replace('،', '').split('.')\n","      li_1 = ['هیچ', 'اصلا', 'هیچگونه']\n","      li_2 = [ 'ندارد', 'نمیتواند']\n","      jomles = []\n","      for sen in sens: \n","        first, second = 0 , 0\n","        flag_1, flag_2 = False, False\n","        tokens = word_tokenize(sen)\n","        jomle = []    \n","        for i in range(len(tokens)):\n","          if tokens[i] in li_1 and flag_1 == False:\n","            first = i\n","            flag_1 = True\n","          if tokens[i] in li_2 and flag_2 == False:\n","            second = i\n","            flag_2 = True\n","        if (second > first) and (flag_1 == True) and (flag_2 == True):\n","          for j in range (first + 1 , second-1 ):\n","            sen = sen.replace(tokens[j], 'NEG_'+tokens[j])\n","        jomles.append(sen)\n","    \n","      jomles = '. '.join(jomles)\n","  \n","      return jomles\n","\n","    def combine_head_and_body(headlines, bodies):\n","        head_and_body = [headline + \" \" + body for i, (headline, body) in\n","                         enumerate(zip(headlines, bodies))]\n","\n","        return head_and_body\n","\n","    def get_vocab(neg_headlines, neg_bodies):\n","        neg_headlines = remove_stopwords(neg_headlines)\n","        neg_bodies = remove_stopwords(neg_bodies)\n","        tf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, use_idf=False,\n","                                        norm='l2')\n","        tf_vectorizer.fit_transform(combine_head_and_body(neg_headlines, neg_bodies))\n","        vocab = tf_vectorizer.vocabulary_\n","\n","        return vocab\n","\n","    def get_features(neg_headlines_test, neg_bodies_test, vocab):\n","        neg_headlines_test = remove_stopwords(neg_headlines_test)\n","        neg_bodies_test = remove_stopwords(neg_bodies_test)\n","        \n","        tf_vectorizer_head = TfidfVectorizer(vocabulary=vocab, use_idf=False, norm='l2')\n","        X_test_head = tf_vectorizer_head.fit_transform(neg_headlines_test)\n","\n","        tf_vectorizer_body = TfidfVectorizer(vocabulary=vocab, use_idf=False, norm='l2')\n","        X_test_body = tf_vectorizer_body.fit_transform(neg_bodies_test)\n","\n","        X_test = np.concatenate([X_test_head.toarray(), X_test_body.toarray()], axis=1)\n","        return X_test\n","\n","#     h, b = get_head_body_tuples(include_holdout=True)\n","#     h_test, b_test = get_head_body_tuples_test()\n","    \n","    h, b = data_train['claim'].tolist() , data_train['body'].tolist()\n","    h_test, b_test = data_test['claim'].tolist(), data_test['body'].tolist()\n","\n","    # Comment out for clean ablation tests\n","    h.extend(h_test)\n","    b.extend(b_test)\n","\n","    neg_headlines_all = [get_negated_text(h) for h in h]\n","    neg_bodies_all = [get_negated_text(b) for b in b]\n","    neg_headlines = [get_negated_text(h) for h in headlines]\n","    neg_bodies = [get_negated_text(b) for b in bodies]\n","\n","    vocab = get_vocab(neg_headlines_all, neg_bodies_all)\n","    X_train = get_features(neg_headlines, neg_bodies, vocab)\n","\n","    return X_train"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"0KlrvAVTrQ-R","executionInfo":{"status":"ok","timestamp":1665212652592,"user_tz":-210,"elapsed":789,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["# x = negated_context_word_12grams_concat_tf5000_l2_all_data(clean_claim, clean_body)\n","# this line is commented in the main notebook"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"TqAM-p6QcGXe","executionInfo":{"status":"ok","timestamp":1665212652594,"user_tz":-210,"elapsed":28,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def char_3grams_5000_concat_all_data(headlines, bodies):\n","\n","    def combine_head_and_body(headlines, bodies):\n","        return [headline + \" \" + body for i, (headline, body) in\n","                tqdm(enumerate(zip(headlines, bodies)))]\n","\n","    # Load train data into CountVectorizer, get the resulting X-values and also the vocabulary\n","    # for the test data feature creation\n","    def get_features(headlines, bodies, headlines_all, bodies_all):\n","        # create vocab on basis of training data\n","        head_and_body = combine_head_and_body(headlines_all, bodies_all)\n","        head_and_body_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), lowercase=True,\n","                                              max_features=5000, use_idf=False, norm='l2')\n","        head_and_body_tfidf.fit(head_and_body)\n","        vocab = head_and_body_tfidf.vocabulary_\n","\n","        # create training feature vectors\n","        X_train_head_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), lowercase=True,\n","                                             stop_words='english', vocabulary=vocab, use_idf=False, norm='l2')\n","        X_train_head = X_train_head_tfidf.fit_transform(headlines)\n","\n","        X_train_body_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), lowercase=True,\n","                                             stop_words='english', vocabulary=vocab, use_idf=False, norm='l2')\n","        X_train_body = X_train_body_tfidf.fit_transform(bodies)\n","\n","        X_train = np.concatenate([X_train_head.toarray(), X_train_body.toarray()], axis=1)\n","\n","        return X_train\n","\n","    h, b = data_train['claim'].tolist() , data_train['body'].tolist()\n","    h_test, b_test = data_test['claim'].tolist(), data_test['body'].tolist()\n","\n","    # Comment out for clean ablation tests\n","    h.extend(h_test)\n","    b.extend(b_test)\n","\n","    X_train = get_features(headlines, bodies, h, b)\n","\n","    return X_train"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"Lb5fB-Eg5pB6","executionInfo":{"status":"ok","timestamp":1665212657058,"user_tz":-210,"elapsed":4491,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a275be71-5fff-48dc-c6e4-2488e2b605a4"},"outputs":[{"output_type":"stream","name":"stderr","text":["1997it [00:00, 294484.59it/s]\n"]}],"source":["x = char_3grams_5000_concat_all_data(clean_claim, clean_body)"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"1bkNyP0i58mE","executionInfo":{"status":"ok","timestamp":1665212657068,"user_tz":-210,"elapsed":43,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"af74ab05-c8a4-4de4-ec9d-37e33a028db7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1997, 10000)"]},"metadata":{},"execution_count":41}],"source":["x.shape"]},{"cell_type":"markdown","metadata":{"id":"yPN7xgRFi2V-"},"source":["## embedding"]},{"cell_type":"markdown","metadata":{"id":"8Q8n3pQ-o4Wh"},"source":["### word embedding persian"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"Y6K7iuEgJIhT","executionInfo":{"status":"ok","timestamp":1665212657072,"user_tz":-210,"elapsed":41,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["import gzip"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"6BQ1bFu7UBdk","executionInfo":{"status":"ok","timestamp":1665212657074,"user_tz":-210,"elapsed":40,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def load_embedding_pandas(FILE, type=\"w2v\"):\n","  embeddings_index=dict()\n","  f = open(FILE)\n","  for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","  f.close()\n","  print('Loaded %s word vectors.' % len(embeddings_index))\n","  return embeddings_index"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"dXcSLm-SUmKh","executionInfo":{"status":"ok","timestamp":1665212817544,"user_tz":-210,"elapsed":160509,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4461b54e-b61e-452d-b3eb-77a421df714e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 2000000 word vectors.\n"]}],"source":["GloVe_vectors = load_embedding_pandas(\"/content/drive/My Drive/Stance Detection Project/cc.fa.300.vec\")\n","# takes a little time...don't worry!"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"omDwJqfYbRGU","executionInfo":{"status":"ok","timestamp":1665212855404,"user_tz":-210,"elapsed":37881,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["g_vec = pd.DataFrame.from_dict(GloVe_vectors)\n","# takes a little time...don't worry!"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"5skj76c3apYu","executionInfo":{"status":"ok","timestamp":1665212855406,"user_tz":-210,"elapsed":28,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["import zipfile\n","import numpy as np\n","import os.path as path\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pickle\n","import nltk\n","from tqdm import tqdm\n","\n","\n","FEATURES_DIR = '/content/drive/My Drive/Stance Detection Project/dataset/features/'\n","EMBEDDINGS_DIR = \"/content/drive/My Drive/Stance Detection Project/dataset/embedding/\" \n","DATA_PATH = \"/content/drive/My Drive/Stance Detection Project/dataset/\"\n","SPLITS_DIR = \"/content/drive/My Drive/Stance Detection Project/dataset/splits/\"\n","\n","def create_embedding_lookup_pandas(text_list, max_nb_words, embedding_dim, embedding,\n","                            embedding_lookup_name, embedding_vocab_name, rdm_emb_init=False, add_unknown=False, tokenizer=None, init_zeros = False):\n","    \"\"\"\n","    Creates the claim embedding lookup table if it not already exists and returns the vocabulary for it\n","    :param text_list:\n","    :param max_nb_words:\n","    :param embedding_dim:\n","    :param GloVe_vectors:\n","    :param embedding_lookup_name:\n","    :param embedding_vocab_name:\n","    :return:\n","    \"\"\"\n","    #del GloVe_vectors\n","\n","    # if ...embedding.npy or vocab.pkl files don't exist: \n","    if not path.exists(FEATURES_DIR + embedding_lookup_name) or not path.exists(FEATURES_DIR + embedding_vocab_name):\n","        print(\"can't find npy or pkl file!\")\n","        vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=None, tokenizer=tokenizer,\n","                                            max_features=max_nb_words, use_idf=True)\n","        vectorizer.fit_transform(text_list)\n","        vocab = vectorizer.vocabulary_\n","\n","\n","        # do not use 0 since we want to use masking in the LSTM later on\n","        for word in vocab.keys():\n","            vocab[word] += 1\n","        if add_unknown == True:\n","            max_index = max(vocab.values())\n","            vocab[\"UNKNOWN\"] = max_index+1\n","\n","        # prepare embedding - create matrix that holds the glove vector for each vocab entry\n","        if rdm_emb_init == True:\n","            embedding_lookup = np.random.random((len(vocab) + 1, embedding_dim))\n","            zero_vec = np.zeros((embedding_dim))\n","            embedding_lookup[0] = zero_vec # for masking\n","        else:\n","            embedding_lookup = np.zeros((len(vocab) + 1, embedding_dim))\n","\n","        if init_zeros == False:\n","            for word, i in vocab.items():\n","                if word == \"UNKNOWN\":\n","                    embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n","                    #print(embedding_vector)\n","                else:\n","                    try:\n","                        embedding_vector = embedding.loc[word].as_matrix()\n","                    except KeyError: #https://stackoverflow.com/questions/15653966/ignore-keyerror-and-continue-program\n","                        continue\n","                if embedding_vector is not None:\n","                    # words not found in embedding index will be all-zeros.\n","                    embedding_lookup[i] = embedding_vector\n","        print(\"created embedding lookup!\")\n","        #print(embedding_lookup[-1])\n","        # save embedding matrix\n","        np.save(FEATURES_DIR + embedding_lookup_name, embedding_lookup)\n","        print(\"embedding matrix saved!\")\n","        # save vocab\n","        with open(FEATURES_DIR + embedding_vocab_name, 'wb') as f:\n","            pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n","        print(\"vocab saved!\")\n","\n","        print(\"Embedding lookup table shape for \" + embedding_lookup_name + \" is: \" + str(embedding_lookup.shape))\n","    #if both .npy and .pkl files exist:\n","    else:\n","        print(\"found npy and pkl files!\")\n","        with open(FEATURES_DIR + embedding_vocab_name, \"rb\") as f:\n","            vocab = pickle.load(f)\n","\n","    print(\"Vocab size for \" + embedding_vocab_name + \" is: \" + str(len(vocab)))\n","\n","    return vocab\n","\n","def text_to_sequences_fixed_size(texts, vocab, MAX_SENT_LENGTH, save_full_text=False, take_full_claim = False):\n","    \"\"\"\n","    Turns sentences of claims into sequences of indices provided by the given vocab.\n","    Unknown words will get an extra index, if\n","    the vocab has a token \"UNKNOWN\". The method takes the longest sentence of the claims, if the\n","    claim should have more than one sentence.\n","    :param texts:\n","    :param vocab:\n","    :param MAX_SENT_LENGTH:\n","    :return:\n","    \"\"\"\n","    data = np.zeros((len(texts), MAX_SENT_LENGTH), dtype='int32')\n","\n","    claims = []\n","    if take_full_claim == False:\n","        for claim in texts:\n","            claim_sents = nltk.sent_tokenize(claim)\n","            word_count_fct = lambda sentence: len(nltk.word_tokenize(sentence)) # take longest sentence of claim if it has more than one\n","            claims.append(max(claim_sents, key=word_count_fct))\n","    else:\n","        claims = texts\n","\n","    data_string_dict = {}\n","    for j, claim in tqdm(enumerate(claims)):\n","        claim_tokens = nltk.word_tokenize(claim.lower())\n","\n","        data_string = \"\"\n","        if save_full_text == True:\n","            for token in claim_tokens:\n","                data_string += token + \" \"\n","            data_string = data_string[:-1]\n","            data_string_dict[j] = data_string\n","\n","        for i, token in enumerate(claim_tokens):\n","            if i < MAX_SENT_LENGTH:\n","                index = vocab.get(token, \"UNKNOWN\")\n","                if index == \"UNKNOWN\":\n","                    index = vocab.get(index, None)\n","                if index != None:\n","                    data[j, i] = index\n","\n","    if save_full_text == True:\n","        return data, data_string_dict\n","    else:\n","        return data\n"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"V9uMrbpUYE9m","executionInfo":{"status":"ok","timestamp":1665212856143,"user_tz":-210,"elapsed":760,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c6948a9f-424b-4890-a5f3-0e792dec82ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":47}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"xNEMxYF06BWf","executionInfo":{"status":"ok","timestamp":1665212856144,"user_tz":-210,"elapsed":7,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def single_flat_LSTM_50d_100(headlines, bodies, GloVe_vectors):\n","\n","    #########################\n","    # PARAMETER DEFINITIONS #\n","    #########################\n","    method_name = \"single_flat_LSTM_50d_100\"\n","    # location path for features\n","    FEATURES_DIR = '/content/drive/My Drive/Stance Detection Project/dataset/features/'\n","    PARAM_DICT_FILENAME = method_name+\"_param_dict.pkl\"\n","\n","    param_dict = {\n","        \"MAX_NB_WORDS\": 50000,  # size of the vocabulary\n","\n","        # sequence lengths\n","        \"MAX_SEQ_LENGTH\": 100, #1000\n","\n","        # embedding specific values\n","        \"EMBEDDING_DIM\": 50,  # dimension of the GloVe embeddings\n","        \"GLOVE_ZIP_FILE\": \"cc.fa.300.vec.gz\",  #*********************\n","        \"GLOVE_FILE\": \"cc.fa.300.vec\",  #*********************\n","\n","        # embedding file names\n","        \"EMBEDDING_FILE\": method_name+\"_embedding.npy\",\n","\n","        # vocab file names\n","        \"VOCAB_FILE\": method_name+\"_vocab.pkl\",\n","    }\n","\n","\n","    ###############################################\n","    # GET VOCABULARY AND PREPARE EMBEDDING MATRIX #\n","    ###############################################\n","\n","    # load GloVe embeddings\n","    # load the whole embedding into memory\n","    \n","#     GloVe_vectors = load_embedding_pandas(param_dict[\"GLOVE_FILE\"])\n","    \n","\n","    # load all claims, orig_docs and evidences\n","    all_heads, all_bodies = data_train['claim'].tolist() , data_train['body'].tolist()\n","    all = all_heads\n","    all.extend(all_bodies)\n","   \n","    \n","\n","    # Comment out for clean ablation checks\n","    # add the unlabeled test data words to the BoW of test+train+holdout data\n","    h_unlbled_test, b_unlbled_test = data_test['claim'].tolist(), data_test['body'].tolist()\n","    all.extend(h_unlbled_test)\n","    all.extend(b_unlbled_test)\n","    \n","\n","    # create and save the embedding matrices for claims, orig_docs and evidences\n","    vocab = create_embedding_lookup_pandas(all, param_dict[\"MAX_NB_WORDS\"], param_dict[\"EMBEDDING_DIM\"],\n","                                           GloVe_vectors, param_dict[\"EMBEDDING_FILE\"], param_dict[\"VOCAB_FILE\"], init_zeros=False,\n","                                           add_unknown=True, rdm_emb_init=True, tokenizer=nltk.word_tokenize)\n","\n","    # unload GloVe_vectors in order to make debugging possible\n","    del GloVe_vectors\n","\n","\n","    #################################################\n","    # Create sequences and embedding for the claims #\n","    #################################################\n","    print(\"Create sequences and embedding for the heads\")\n","\n","    concatenated = []\n","    for i in range(len(headlines)):\n","        concatenated.append(headlines[i] + \". \" + bodies[i])\n","\n","    # replace tokens of claims by vocabulary ids - the ids refer to the index of the embedding matrix which holds the word embedding for this vocab word\n","    sequences = text_to_sequences_fixed_size(concatenated, vocab, param_dict[\"MAX_SEQ_LENGTH\"], save_full_text=False,\n","                                             take_full_claim=True)\n","\n","\n","\n","    #################################################\n","    # SAVE PARAM_DICT AND CONCATENATE TRAINING DATA #\n","    #################################################\n","\n","    # save param_dict\n","    with open(FEATURES_DIR+PARAM_DICT_FILENAME, 'wb') as f:\n","        pickle.dump(param_dict, f, pickle.HIGHEST_PROTOCOL)\n","    print(\"Save PARAM_DICT as \" + FEATURES_DIR+PARAM_DICT_FILENAME)\n","\n","    return sequences"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"0iYxaWtCXjCF","executionInfo":{"status":"ok","timestamp":1665212860854,"user_tz":-210,"elapsed":4717,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c481047f-1583-47c1-f164-2253f5c56e39"},"outputs":[{"output_type":"stream","name":"stdout","text":["found npy and pkl files!\n","Vocab size for single_flat_LSTM_50d_100_vocab.pkl is: 48873\n","Create sequences and embedding for the heads\n"]},{"output_type":"stream","name":"stderr","text":["1997it [00:04, 455.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Save PARAM_DICT as /content/drive/My Drive/Stance Detection Project/dataset/features/single_flat_LSTM_50d_100_param_dict.pkl\n"]}],"source":["# aa = single_flat_LSTM_50d_100(docs, docs_2, g_vec) \n","# in the main code(the line above this) docs and docs_2 are not defined so we will try this:\n","aa = single_flat_LSTM_50d_100(clean_claim, clean_body, g_vec)\n","# this is for testing the function"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"UiD6GBFmr-aK","executionInfo":{"status":"ok","timestamp":1665212860855,"user_tz":-210,"elapsed":30,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["#!gunzip \"/content/drive/My Drive/persian_stance_baseline_data/vectors/cc.fa.300.vec.gz\"\n","# this line is commented in the main notebook"]},{"cell_type":"markdown","metadata":{"id":"oHFR3Mqyi-EF"},"source":["## generate feature"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"URLSI-k5h2OH","executionInfo":{"status":"ok","timestamp":1665212860859,"user_tz":-210,"elapsed":33,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9125994a-9d35-421e-dca0-30c6f08ec740"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['claim', 'body', 'label'], dtype='object')"]},"metadata":{},"execution_count":51}],"source":["dataset_clean.columns"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"l_Pz8dQUbElO","executionInfo":{"status":"ok","timestamp":1665212860862,"user_tz":-210,"elapsed":25,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["bow_feats = ['hand', 'negated_context_word_12grams_concat_tf5000_l2_all_data', 'char_3grams_5000_concat_all_data']\n","\n","word_emb = ['single_flat_LSTM_50d_100']\n","\n","#topic_models = ['latent_dirichlet_allocation_300', 'latent_semantic_indexing_gensim_300_concat', 'NMF_fit_all_concat_300_no_holdout', 'NMF_cos_300']\n","# topic_models = ['latent_dirichlet_allocation_300', 'latent_semantic_indexing_gensim_300_concat_holdout', 'NMF_fit_all_concat_300_and_test', 'NMF_cos_300']"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"etKS3L3WhF58","executionInfo":{"status":"ok","timestamp":1665212860864,"user_tz":-210,"elapsed":26,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def gen_or_load_feats(feat_fn, headlines, bodies, feature_file, feature):\n","    if not os.path.isfile(feature_file):\n","        if 'single_flat_LSTM_50d_100' in feature:\n","            feats = feat_fn(headlines, bodies, g_vec)\n","        else:\n","            feats = feat_fn(headlines, bodies)\n","        np.save(feature_file, feats)\n","\n","    return np.load(feature_file)"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"9ohJHd6qgCDt","executionInfo":{"status":"ok","timestamp":1665212860865,"user_tz":-210,"elapsed":27,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def generate_features(dataset, name, feature_list, features_dir):\n","    \"\"\"\n","    Creates feature vectors out of the provided dataset\n","    \"\"\"\n","    h, b, y = [], [], []\n","\n","    feature_dict = {'hand': hand_features,\n","                    'single_flat_LSTM_50d_100': single_flat_LSTM_50d_100,\n","                    'char_3grams_5000_concat_all_data': char_3grams_5000_concat_all_data,\n","                    'negated_context_word_12grams_concat_tf5000_l2_all_data': negated_context_word_12grams_concat_tf5000_l2_all_data,\n","                    }\n","    \n","    y = dataset['label'].tolist()\n","    h = dataset['claim'].tolist()\n","    b = dataset['body'].tolist()\n","\n","    X_feat = []\n","    feat_list = []\n","    last_index = 0\n","    for feature in feature_list:\n","        print(\"processing \" + feature)\n","        feat = gen_or_load_feats(feature_dict[feature], h, b, features_dir+\"/\"+feature+\".\"+name+'.npy', feature)\n","        feat_list.append((last_index, last_index+len(feat[0]), str(feature)))\n","        last_index += len(feat[0])\n","        X_feat.append(feat)\n","    print(\"done with processing all features in feature_list\")\n","    X = np.concatenate(X_feat, axis=1)\n","\n","    return X, y, feat_list"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"H3AzTfruhIhU","executionInfo":{"status":"ok","timestamp":1665212860867,"user_tz":-210,"elapsed":28,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["features_dir = '/content/drive/My Drive/Stance Detection Project/dataset/features'\n","feature_list = word_emb + bow_feats\n","name = 'first_1'"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"OHTazG37hJ_r","executionInfo":{"status":"ok","timestamp":1665212860867,"user_tz":-210,"elapsed":28,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"abfc0b98-5057-4e28-c856-6f562535b370"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['single_flat_LSTM_50d_100',\n"," 'hand',\n"," 'negated_context_word_12grams_concat_tf5000_l2_all_data',\n"," 'char_3grams_5000_concat_all_data']"]},"metadata":{},"execution_count":56}],"source":["feature_list"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"eyBjbFoBhiMw","executionInfo":{"status":"ok","timestamp":1665212866756,"user_tz":-210,"elapsed":5914,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"81fd43e9-e9fa-4c71-e470-994e04685707"},"outputs":[{"output_type":"stream","name":"stdout","text":["processing single_flat_LSTM_50d_100\n","processing hand\n","processing negated_context_word_12grams_concat_tf5000_l2_all_data\n","processing char_3grams_5000_concat_all_data\n","done with processing all features in feature_list\n"]}],"source":["X, y , feature_list = generate_features(dataset_clean, name, feature_list, features_dir)\n","# this code takes a very very long time to finish execution if the files required are not generated before...don't worry!"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"Vsw4tRMW2Dhh","executionInfo":{"status":"ok","timestamp":1665212866758,"user_tz":-210,"elapsed":34,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4a545089-402b-4326-f579-cdc7de956a94"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 100, 'single_flat_LSTM_50d_100'),\n"," (100, 126, 'hand'),\n"," (126, 10126, 'negated_context_word_12grams_concat_tf5000_l2_all_data'),\n"," (10126, 20126, 'char_3grams_5000_concat_all_data')]"]},"metadata":{},"execution_count":58}],"source":["feature_list"]},{"cell_type":"markdown","metadata":{"id":"r69G5MZ-ZyNs"},"source":["# Base lines"]},{"cell_type":"markdown","metadata":{"id":"d7M8JN4WaYwz"},"source":["majority vote"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"LtXP3tKXYlGF","executionInfo":{"status":"ok","timestamp":1665212866759,"user_tz":-210,"elapsed":32,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    dataset_clean['claim'], dataset_clean['label'], test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"EppwBAJrXEMu","executionInfo":{"status":"ok","timestamp":1665212866761,"user_tz":-210,"elapsed":32,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["from sklearn.dummy import DummyClassifier\n","\n","clf_maj = DummyClassifier(strategy=\"most_frequent\")\n","\n","clf_maj.fit(X_train, y_train)\n","\n","y_pred_maj = clf_maj.predict(X_test)"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"qgFMeiAsg_Hl","executionInfo":{"status":"ok","timestamp":1665212866762,"user_tz":-210,"elapsed":32,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["LABELS = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n","LABELS_RELATED = ['unrelated','related']\n","RELATED = LABELS[0:3]\n","\n","def score_submission(gold_labels, test_labels):\n","    score = 0.0\n","    cm = [[0, 0, 0, 0],\n","          [0, 0, 0, 0],\n","          [0, 0, 0, 0],\n","          [0, 0, 0, 0]]\n","\n","    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n","        g_stance, t_stance = g, t\n","        if g_stance == t_stance:\n","            score += 0.25\n","            if g_stance != 'unrelated':\n","                score += 0.50\n","        if g_stance in RELATED and t_stance in RELATED:\n","            score += 0.25\n","\n","        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n","\n","    return score, cm"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"TW1IOOYwZKLi","executionInfo":{"status":"ok","timestamp":1665212866762,"user_tz":-210,"elapsed":31,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3287ff2-0686-4abd-b32b-08c20561152f"},"outputs":[{"output_type":"stream","name":"stdout","text":["FNC-1 score from restored model: 0.6280487804878049\n","\n"]}],"source":["score, cm = score_submission(y_test, y_pred_maj)\n","fold_score, _ = score_submission(y_test, y_pred_maj)\n","max_fold_score, _ = score_submission(y_test, y_test)\n","score = fold_score / max_fold_score\n","\n","print(\"FNC-1 score from restored model: \" +  str(score) +\"\\n\")"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"ylIEsw6ThrYz","executionInfo":{"status":"ok","timestamp":1665212866762,"user_tz":-210,"elapsed":27,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def print_confusion_matrix(cm):\n","    lines = []\n","    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n","    line_len = len(header)\n","    lines.append(\"-\"*line_len)\n","    lines.append(header)\n","    lines.append(\"-\"*line_len)\n","\n","    hit = 0\n","    total = 0\n","    for i, row in enumerate(cm):\n","        hit += row[i]\n","        total += sum(row)\n","        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n","                                                                   *row))\n","        lines.append(\"-\"*line_len)\n","    print('\\n'.join(lines))"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"oIn8-WFuYC3B","executionInfo":{"status":"ok","timestamp":1665212866763,"user_tz":-210,"elapsed":28,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"559eb3c9-3c40-4bef-8fde-20fa6e503447"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------------------------\n","|           |   Agree   | Disagree  |  Discuss  | Unrelated |\n","-------------------------------------------------------------\n","|   Agree   |     0     |     0     |    22     |     0     |\n","-------------------------------------------------------------\n","| Disagree  |     0     |     0     |    37     |     0     |\n","-------------------------------------------------------------\n","|  Discuss  |     0     |     0     |    217    |     0     |\n","-------------------------------------------------------------\n","| Unrelated |     0     |     0     |    124    |     0     |\n","-------------------------------------------------------------\n"]}],"source":["print_confusion_matrix(cm)"]},{"cell_type":"markdown","metadata":{"id":"YrJQpoP8Z1xx"},"source":["# Model "]},{"cell_type":"code","execution_count":65,"metadata":{"id":"Jm7gN_tDXQpF","executionInfo":{"status":"ok","timestamp":1665212866763,"user_tz":-210,"elapsed":25,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def split_X(X_train, MAX_SEQ_LENGTH_HEADS):\n","    # split to get [heads, docs]\n","    X_train_splits = np.hsplit(X_train, np.array([MAX_SEQ_LENGTH_HEADS]))\n","    X_train_head = X_train_splits[0]\n","    X_train_doc = X_train_splits[1]\n","\n","    print(\"X_train_head.shape = \" + str(np.array(X_train_head).shape))\n","    print(\"X_train_doc.shape = \" + str(np.array(X_train_doc).shape))\n","\n","    return X_train_head,X_train_doc"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"CxDn3uG42KQD","executionInfo":{"status":"ok","timestamp":1665212870746,"user_tz":-210,"elapsed":4007,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["import numpy as np\n","import os.path as path\n","import pickle\n","\n","from keras.layers.core import Dense\n","# from keras.layers.pooling import GlobalMaxPooling1D\n","from keras.layers.recurrent import LSTM\n","from keras import optimizers\n","# from fnc.models.Keras_utils import EarlyStoppingOnF1, convert_data_to_one_hot, calculate_class_weight, split_X\n","# from fnc.models.keras_custom_layers.attention_custom import *\n","from keras.models import Model, load_model\n","from keras.layers.merge import concatenate\n","from keras.layers import Embedding, Input\n"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"BV_gZl423WL0","executionInfo":{"status":"ok","timestamp":1665212870758,"user_tz":-210,"elapsed":122,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"kphZuuiS7xMe","executionInfo":{"status":"ok","timestamp":1665212870768,"user_tz":-210,"elapsed":128,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def convert_data_to_one_hot(y_train):\n","    \n","    y_train_temp = np.zeros((y_train.size, y_train.max() + 1), dtype=np.int)\n","    y_train_temp[np.arange(y_train.size), y_train] = 1\n","\n","    return y_train_temp"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"05-6Ix2L8EkU","executionInfo":{"status":"ok","timestamp":1665212870769,"user_tz":-210,"elapsed":125,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5c0ae677-f3f7-4f0e-acf4-da736b5186d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Agree', 'Disagree', 'Discuss', 'Unrelated']\n","['Agree', 'Disagree', 'Discuss', 'Unrelated']\n"]}],"source":["from sklearn import preprocessing\n","import keras\n","\n","le = preprocessing.LabelEncoder()\n","le.fit(y_train)\n","print(list(le.classes_))\n","y_train = le.transform(y_train)\n","\n","le = preprocessing.LabelEncoder()\n","le.fit(y_test)\n","print(list(le.classes_))\n","y_test = le.transform(y_test)"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"TNNYZbzY5Jjs","executionInfo":{"status":"ok","timestamp":1665212870770,"user_tz":-210,"elapsed":118,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def calculate_class_weight(y_train, no_classes=2):\n","    # https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n","    from sklearn.utils import class_weight\n","\n","    class_weight_list = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train), y = y_train)\n","    class_weights = {}\n","    for i in range(no_classes):\n","        class_weights[i] = class_weight_list[i]\n","    print(class_weights)\n","    return class_weights"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"K3K_ngFR2Obp","executionInfo":{"status":"ok","timestamp":1665212871648,"user_tz":-210,"elapsed":994,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b347b8f3-7e6e-4c96-8149-6f6bf5fcdfe5"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train_head.shape = (1597, 100)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]},{"output_type":"stream","name":"stdout","text":["X_train_doc.shape = (1597, 20026)\n","X_train_head.shape = (400, 100)\n","X_train_doc.shape = (400, 20026)\n","EMBEDDING_FILE.shape = (48874, 50)\n","{0: 3.4717391304347824, 1: 2.3624260355029585, 2: 0.4691539365452409, 3: 0.8641774891774892}\n"]}],"source":["y_train_one_hot = convert_data_to_one_hot(y_train)\n","y_test_one_hot = convert_data_to_one_hot(y_test)\n","\n","param_dict=\"single_flat_LSTM_50d_100\"\n","\n","FEATURES_DIR = '/content/drive/My Drive/Stance Detection Project/dataset/features/'\n","\n","PARAM_DICT_FILENAME = param_dict + \"_param_dict.pkl\"\n","\n","\n","# load feature dict for LSTM_1000_GloVe\n","with open(FEATURES_DIR+PARAM_DICT_FILENAME, \"rb\") as f:\n","  param_dict = pickle.load(f)\n","\n","# load parameters needed for embedding layer\n","EMBEDDING_DIM = param_dict[\"EMBEDDING_DIM\"] # e.g. 50\n","MAX_SEQ_LENGTH = param_dict[\"MAX_SEQ_LENGTH\"] # e.g. 100\n","\n","X_train_LSTM, X_train_MLP = split_X(X_train, MAX_SEQ_LENGTH)\n","X_test_LSTM, X_test_MLP = split_X(X_test, MAX_SEQ_LENGTH)\n","\n","# load embeddings\n","EMBEDDING_FILE = np.load(FEATURES_DIR+param_dict[\"EMBEDDING_FILE\"])\n","\n","print(\"EMBEDDING_FILE.shape = \" + str(EMBEDDING_FILE.shape))\n","\n","# calc cass weights\n","class_weights = calculate_class_weight(y_train, no_classes=4)"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"m2ZdZpGm5TRs","executionInfo":{"status":"ok","timestamp":1665212872872,"user_tz":-210,"elapsed":1238,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe2823c8-5362-485e-9e54-8c5a9772577f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," lstm_input (InputLayer)        [(None, 100)]        0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 100, 50)      2443700     ['lstm_input[0][0]']             \n","                                                                                                  \n"," lstm (LSTM)                    (None, 100, 100)     60400       ['embedding[0][0]']              \n","                                                                                                  \n"," lstm_1 (LSTM)                  (None, 100)          80400       ['lstm[0][0]']                   \n","                                                                                                  \n"," mlp_input (InputLayer)         [(None, 20026)]      0           []                               \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 20126)        0           ['lstm_1[0][0]',                 \n","                                                                  'mlp_input[0][0]']              \n","                                                                                                  \n"," dense (Dense)                  (None, 600)          12076200    ['concatenate[0][0]']            \n","                                                                                                  \n"," dense_1 (Dense)                (None, 600)          360600      ['dense[0][0]']                  \n","                                                                                                  \n"," dense_2 (Dense)                (None, 600)          360600      ['dense_1[0][0]']                \n","                                                                                                  \n"," dense_out (Dense)              (None, 4)            2404        ['dense_2[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 15,384,304\n","Trainable params: 15,384,304\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["kernel_initializer = 'glorot_uniform'\n","regularizer = None\n","batch_size=200\n","dense_activity_regularizer=None\n","LSTM_implementation = 2\n","\n","\n","################\n","# CLAIMS LAYER #\n","################\n","lstm_input = Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name='lstm_input') # receive sequences of MAX_SEQ_LENGTH_CLAIMS integers\n","embedding = Embedding(input_dim=len(EMBEDDING_FILE), # lookup table size\n","                                    output_dim=EMBEDDING_DIM, # output dim for each number in a sequence\n","                                    weights=[EMBEDDING_FILE],\n","                                    input_length=MAX_SEQ_LENGTH, # receive sequences of MAX_SEQ_LENGTH_CLAIMS integers\n","                                    mask_zero=True,\n","                                    trainable=True)(lstm_input)\n","data_LSTM = LSTM(\n","            100, return_sequences=True, stateful=False, dropout=0.2,\n","            batch_input_shape=(batch_size, MAX_SEQ_LENGTH, EMBEDDING_DIM),\n","            input_shape=(MAX_SEQ_LENGTH, EMBEDDING_DIM), implementation=LSTM_implementation\n","            )(embedding)\n","data_LSTM = LSTM(\n","            100, return_sequences=False, stateful=False, dropout=0.2,\n","            batch_input_shape=(batch_size, MAX_SEQ_LENGTH, EMBEDDING_DIM),\n","            input_shape=(MAX_SEQ_LENGTH, EMBEDDING_DIM), implementation=LSTM_implementation\n","            )(data_LSTM)\n","\n","###############################\n","# MLP (NON-TIMESTEP) FEATURES #\n","###############################\n","mlp_input = Input(shape=(len(X_train_MLP[0]),), dtype='float32', name='mlp_input')\n","\n","###############\n","# MERGE LAYER #\n","###############\n","merged = concatenate([data_LSTM, mlp_input])\n","\n","dense_mid = Dense(600, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n","                          activity_regularizer=dense_activity_regularizer, activation='relu')(merged)\n","dense_mid = Dense(600, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n","                          activity_regularizer=dense_activity_regularizer, activation='relu')(dense_mid)\n","dense_mid = Dense(600, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n","                          activity_regularizer=dense_activity_regularizer, activation='relu')(dense_mid)\n","dense_out = Dense(4,activation='softmax', name='dense_out')(dense_mid)\n","\n","# build model\n","model = Model(inputs=[lstm_input, mlp_input], outputs=[dense_out])\n","\n","# print summary\n","model.summary()\n"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"qVH4fd1KDJzm","executionInfo":{"status":"ok","timestamp":1665212872874,"user_tz":-210,"elapsed":26,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["LABELS = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n","LABELS_RELATED = ['unrelated','related']\n","RELATED = LABELS[0:3]\n","\n","def score_submission(gold_labels, test_labels):\n","    score = 0.0\n","    cm = [[0, 0, 0, 0],\n","          [0, 0, 0, 0],\n","          [0, 0, 0, 0],\n","          [0, 0, 0, 0]]\n","\n","    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n","        g_stance, t_stance = g, t\n","        if g_stance == t_stance:\n","            score += 0.25\n","            if g_stance != 'unrelated':\n","                score += 0.50\n","        if g_stance in RELATED and t_stance in RELATED:\n","            score += 0.25\n","\n","        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n","\n","    return score, cm"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"zzm0UREZC3fN","executionInfo":{"status":"ok","timestamp":1665212872876,"user_tz":-210,"elapsed":25,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["from keras.callbacks import Callback\n","import numpy as np\n","class EarlyStoppingOnF1(Callback):\n","    \"\"\"\n","    Prints some metrics after each epoch in order to observe overfitting\n","                https://github.com/fchollet/keras/issues/5794\n","                custom metrics: https://github.com/fchollet/keras/issues/2607\n","    \"\"\"\n","\n","    def __init__(self, epochs,\n","                 X_test_claims,\n","                 X_test_orig_docs,\n","                 y_test, loss_filename, epsilon=0.0, min_epoch = 15, X_test_nt=None):\n","        self.epochs = epochs\n","        self.patience = 2\n","        self.counter = 0\n","        self.prev_score = 0\n","        self.epsilon = epsilon\n","        self.loss_filename = loss_filename\n","        self.min_epoch = min_epoch\n","        self.X_test_nt = X_test_nt\n","        #self.print_train_f1 = print_train_f1\n","\n","        #self.X_train_claims = X_train_claims\n","        #self.X_train_orig_docs = X_train_orig_docs\n","        #self.X_train_evid = X_train_evid\n","        #self.y_train = y_train\n","\n","        self.X_test_claims = X_test_claims\n","        self.X_test_orig_docs = X_test_orig_docs\n","        self.y_test = y_test\n","        Callback.__init__(self)\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        if epoch + 1 < self.epochs:\n","            from sklearn.metrics import f1_score\n","\n","            # get prediction and convert into list\n","            if type(self.X_test_orig_docs).__module__ == np.__name__ and type(self.X_test_nt).__module__ == np.__name__:\n","                predicted_one_hot = self.model.predict([\n","                    self.X_test_claims,\n","                    self.X_test_orig_docs,\n","                    self.X_test_nt\n","                ])\n","            elif type(self.X_test_orig_docs).__module__ == np.__name__:\n","                predicted_one_hot = self.model.predict([\n","                    self.X_test_claims,\n","                    self.X_test_orig_docs,\n","                ])\n","            else:\n","                predicted_one_hot = self.model.predict(self.X_test_claims)\n","            predict = np.argmax(predicted_one_hot, axis=-1)\n","\n","            \"\"\"\n","            predicted_one_hot_train = self.model.predict([self.X_train_claims, self.X_train_orig_docs, self.X_train_evid])\n","            predict_train = np.argmax(predicted_one_hot_train, axis=-1)\n","\n","            \n","            # f1 for train data\n","            f1_macro_train = \"\"\n","            if self.print_train_f1 == True:\n","                f1_0_train = f1_score(self.y_train, predict_train, labels=[0], average=None)\n","                f1_1_train = f1_score(self.y_train, predict_train, labels=[1], average=None)\n","                f1_macro_train = (f1_0_train[0] + f1_1_train[0]) / 2\n","                print(\" - train_f1_(macro): \" + str(f1_macro_train))\"\"\"\n","\n","            predicted = [LABELS[int(a)] for a in predict]\n","            actual = [LABELS[int(a)] for a in self.y_test]\n","            # calc FNC score\n","            fold_score, _ = score_submission(actual, predicted)\n","            max_fold_score, _ = score_submission(actual, actual)\n","            fnc_score = fold_score / max_fold_score\n","            print(\" - fnc_score: \" + str(fnc_score))\n","\n","            # f1 for test data\n","            f1_0 = f1_score(self.y_test, predict, labels=[0], average=None)\n","            f1_1 = f1_score(self.y_test, predict, labels=[1], average=None)\n","            f1_2 = f1_score(self.y_test, predict, labels=[2], average=None)\n","            f1_3 = f1_score(self.y_test, predict, labels=[3], average=None)\n","            f1_macro = (f1_0[0] + f1_1[0] + f1_2[0] + f1_3[0]) / 4\n","            print(\" - val_f1_(macro): \" + str(f1_macro))\n","            print(\"\\n\")\n","\n","            header = \"\"\n","            values = \"\"\n","            for key, value in logs.items():\n","                header = header + key + \";\"\n","                values = values + str(value) + \";\"\n","            if epoch == 0:\n","                values = \"\\n\" + header + \"val_f1_macro;\" + \"fnc_score;\" + \"\\n\" + values + str(f1_macro) + str(fnc_score) + \";\"\n","            else:\n","                values += str(f1_macro) + \";\" + str(fnc_score) + \";\"\n","            append_to_loss_monitor_file(values, self.loss_filename)\n","\n","            if epoch >= self.min_epoch-1:  # 9\n","                if f1_macro + self.epsilon <= self.prev_score:\n","                    self.counter += 1\n","                else:\n","                    self.counter = 0\n","                if self.counter >= 2:\n","                    self.model.stop_training = True\n","            #print(\"Counter at \" + str(self.counter))\n","            self.prev_score = f1_macro\n","            #print(\"\\n\")"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"ubk3uR3FF87-","executionInfo":{"status":"ok","timestamp":1665212872878,"user_tz":-210,"elapsed":25,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def append_to_loss_monitor_file(text, filepath):\n","    with open(filepath, 'a+') as the_file:\n","        the_file.write(text+\"\\n\")"]},{"cell_type":"code","source":["from tensorflow.keras import optimizers\n"],"metadata":{"id":"YRp1geXI12kG","executionInfo":{"status":"ok","timestamp":1665212872879,"user_tz":-210,"elapsed":21,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","execution_count":77,"metadata":{"id":"Xn-ek0kr-wPk","executionInfo":{"status":"ok","timestamp":1665213294050,"user_tz":-210,"elapsed":421189,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"235678a0-a17f-4213-a415-6c617e18b296"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(RMSprop, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Used optimizer: rms, lr=0.001\n","Epoch 1/70\n","8/8 [==============================] - ETA: 0s - loss: 1.6100 - accuracy: 0.2417 - fnc_score: 0.27168021680216803\n"," - val_f1_(macro): 0.1491457459064215\n","\n","\n","8/8 [==============================] - 21s 2s/step - loss: 1.6100 - accuracy: 0.2417 - val_loss: 1.3878 - val_accuracy: 0.1750\n","Epoch 2/70\n","8/8 [==============================] - ETA: 0s - loss: 1.3871 - accuracy: 0.2649 - fnc_score: 0.29132791327913277\n"," - val_f1_(macro): 0.1765347435971107\n","\n","\n","8/8 [==============================] - 13s 2s/step - loss: 1.3871 - accuracy: 0.2649 - val_loss: 1.3828 - val_accuracy: 0.1725\n","Epoch 3/70\n","8/8 [==============================] - ETA: 0s - loss: 1.3560 - accuracy: 0.2461 - fnc_score: 0.6036585365853658\n"," - val_f1_(macro): 0.2073051188290415\n","\n","\n","8/8 [==============================] - 13s 2s/step - loss: 1.3560 - accuracy: 0.2461 - val_loss: 1.2459 - val_accuracy: 0.5225\n","Epoch 4/70\n","8/8 [==============================] - ETA: 0s - loss: 1.3162 - accuracy: 0.4183 - fnc_score: 0.4911924119241192\n"," - val_f1_(macro): 0.2546773724830976\n","\n","\n","8/8 [==============================] - 13s 2s/step - loss: 1.3162 - accuracy: 0.4183 - val_loss: 1.1134 - val_accuracy: 0.4700\n","Epoch 5/70\n","8/8 [==============================] - ETA: 0s - loss: 1.2205 - accuracy: 0.4120 - fnc_score: 0.32113821138211385\n"," - val_f1_(macro): 0.23868433636348174\n","\n","\n","8/8 [==============================] - 13s 2s/step - loss: 1.2205 - accuracy: 0.4120 - val_loss: 1.2889 - val_accuracy: 0.3325\n","Epoch 6/70\n","8/8 [==============================] - ETA: 0s - loss: 1.0485 - accuracy: 0.4684 - fnc_score: 0.46476964769647694\n"," - val_f1_(macro): 0.2354722682268581\n","\n","\n","8/8 [==============================] - 13s 2s/step - loss: 1.0485 - accuracy: 0.4684 - val_loss: 1.3433 - val_accuracy: 0.3800\n","Epoch 7/70\n","8/8 [==============================] - ETA: 0s - loss: 1.0807 - accuracy: 0.5698 - fnc_score: 0.28523035230352306\n"," - val_f1_(macro): 0.18852733180428133\n","\n","\n","8/8 [==============================] - 16s 2s/step - loss: 1.0807 - accuracy: 0.5698 - val_loss: 1.4135 - val_accuracy: 0.3025\n","Epoch 8/70\n","8/8 [==============================] - ETA: 0s - loss: 0.7338 - accuracy: 0.6450 - fnc_score: 0.5860433604336044\n"," - val_f1_(macro): 0.2597437752466194\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.7338 - accuracy: 0.6450 - val_loss: 1.7800 - val_accuracy: 0.4925\n","Epoch 9/70\n","8/8 [==============================] - ETA: 0s - loss: 0.6162 - accuracy: 0.6925 - fnc_score: 0.3218157181571816\n"," - val_f1_(macro): 0.22521414219611569\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.6162 - accuracy: 0.6925 - val_loss: 1.6768 - val_accuracy: 0.2750\n","Epoch 10/70\n","8/8 [==============================] - ETA: 0s - loss: 0.7040 - accuracy: 0.6794 - fnc_score: 0.4715447154471545\n"," - val_f1_(macro): 0.2428696815536297\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.7040 - accuracy: 0.6794 - val_loss: 1.4309 - val_accuracy: 0.4300\n","Epoch 11/70\n","8/8 [==============================] - ETA: 0s - loss: 0.4921 - accuracy: 0.7639 - fnc_score: 0.5101626016260162\n"," - val_f1_(macro): 0.300732975389726\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.4921 - accuracy: 0.7639 - val_loss: 1.6687 - val_accuracy: 0.4425\n","Epoch 12/70\n","8/8 [==============================] - ETA: 0s - loss: 0.4365 - accuracy: 0.7696 - fnc_score: 0.39227642276422764\n"," - val_f1_(macro): 0.2471081174038383\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.4365 - accuracy: 0.7696 - val_loss: 1.9318 - val_accuracy: 0.3650\n","Epoch 13/70\n","8/8 [==============================] - ETA: 0s - loss: 0.2987 - accuracy: 0.8272 - fnc_score: 0.3150406504065041\n"," - val_f1_(macro): 0.23012331583872403\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.2987 - accuracy: 0.8272 - val_loss: 2.4257 - val_accuracy: 0.2975\n","Epoch 14/70\n","8/8 [==============================] - ETA: 0s - loss: 0.3599 - accuracy: 0.8015 - fnc_score: 0.43157181571815717\n"," - val_f1_(macro): 0.25611462289117104\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.3599 - accuracy: 0.8015 - val_loss: 1.9894 - val_accuracy: 0.3925\n","Epoch 15/70\n","8/8 [==============================] - ETA: 0s - loss: 0.4250 - accuracy: 0.8034 - fnc_score: 0.4654471544715447\n"," - val_f1_(macro): 0.26716562174700403\n","\n","\n","8/8 [==============================] - 13s 2s/step - loss: 0.4250 - accuracy: 0.8034 - val_loss: 1.8290 - val_accuracy: 0.3950\n","Epoch 16/70\n","8/8 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.8773 - fnc_score: 0.3224932249322493\n"," - val_f1_(macro): 0.23475271751558857\n","\n","\n","8/8 [==============================] - 16s 2s/step - loss: 0.2492 - accuracy: 0.8773 - val_loss: 2.3954 - val_accuracy: 0.3025\n","Epoch 17/70\n","8/8 [==============================] - ETA: 0s - loss: 0.1740 - accuracy: 0.9092 - fnc_score: 0.38550135501355015\n"," - val_f1_(macro): 0.25965144146754116\n","\n","\n","8/8 [==============================] - 13s 2s/step - loss: 0.1740 - accuracy: 0.9092 - val_loss: 3.0225 - val_accuracy: 0.3300\n","Epoch 18/70\n","8/8 [==============================] - ETA: 0s - loss: 0.6619 - accuracy: 0.7771 - fnc_score: 0.4383468834688347\n"," - val_f1_(macro): 0.27131446732792674\n","\n","\n","8/8 [==============================] - 13s 2s/step - loss: 0.6619 - accuracy: 0.7771 - val_loss: 2.1788 - val_accuracy: 0.3750\n","Epoch 19/70\n","8/8 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9368 - fnc_score: 0.4071815718157182\n"," - val_f1_(macro): 0.25819747509674384\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.1330 - accuracy: 0.9368 - val_loss: 2.8311 - val_accuracy: 0.3525\n","Epoch 20/70\n","8/8 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 0.9180 - fnc_score: 0.4444444444444444\n"," - val_f1_(macro): 0.28588198452478547\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.1630 - accuracy: 0.9180 - val_loss: 2.5425 - val_accuracy: 0.3850\n","Epoch 21/70\n","8/8 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.9305 - fnc_score: 0.4491869918699187\n"," - val_f1_(macro): 0.2896070864816279\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.1348 - accuracy: 0.9305 - val_loss: 2.7578 - val_accuracy: 0.4100\n","Epoch 22/70\n","8/8 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9474 - fnc_score: 0.29336043360433606\n"," - val_f1_(macro): 0.21385960134026416\n","\n","\n","8/8 [==============================] - 13s 2s/step - loss: 0.1292 - accuracy: 0.9474 - val_loss: 4.0387 - val_accuracy: 0.2750\n","Epoch 23/70\n","8/8 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.8986 - fnc_score: 0.510840108401084\n"," - val_f1_(macro): 0.2867153617550769\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.2221 - accuracy: 0.8986 - val_loss: 3.1430 - val_accuracy: 0.4450\n","Epoch 24/70\n","8/8 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9699 - fnc_score: 0.5196476964769647\n"," - val_f1_(macro): 0.2611595964039847\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.0756 - accuracy: 0.9699 - val_loss: 3.7107 - val_accuracy: 0.4325\n","Epoch 25/70\n","8/8 [==============================] - ETA: 0s - loss: 0.2397 - accuracy: 0.8823 - fnc_score: 0.4247967479674797\n"," - val_f1_(macro): 0.2676880867879886\n","\n","\n","8/8 [==============================] - 15s 2s/step - loss: 0.2397 - accuracy: 0.8823 - val_loss: 3.1181 - val_accuracy: 0.3875\n","Epoch 26/70\n","8/8 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9249 - fnc_score: 0.5\n"," - val_f1_(macro): 0.2684819649098743\n","\n","\n","8/8 [==============================] - 13s 2s/step - loss: 0.2220 - accuracy: 0.9249 - val_loss: 4.2174 - val_accuracy: 0.4475\n","Epoch 27/70\n","8/8 [==============================] - ETA: 0s - loss: 0.5497 - accuracy: 0.8885 - fnc_score: 0.46815718157181574\n"," - val_f1_(macro): 0.2880787591684761\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.5497 - accuracy: 0.8885 - val_loss: 2.9988 - val_accuracy: 0.4150\n","Epoch 28/70\n","8/8 [==============================] - ETA: 0s - loss: 0.0585 - accuracy: 0.9756 - fnc_score: 0.47628726287262874\n"," - val_f1_(macro): 0.29007194560073374\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.0585 - accuracy: 0.9756 - val_loss: 3.4582 - val_accuracy: 0.4175\n","Epoch 29/70\n","8/8 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9800 - fnc_score: 0.48848238482384826\n"," - val_f1_(macro): 0.26960862761833637\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.0502 - accuracy: 0.9800 - val_loss: 3.7651 - val_accuracy: 0.4175\n","Epoch 30/70\n","8/8 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9750 - fnc_score: 0.34010840108401086\n"," - val_f1_(macro): 0.24096455313414147\n","\n","\n","8/8 [==============================] - 14s 2s/step - loss: 0.0591 - accuracy: 0.9750 - val_loss: 4.3197 - val_accuracy: 0.3250\n"]}],"source":["lr = 0.001\n","optimizer_name = \"rms\"\n","use_class_weights = True\n","epochs = 70\n","min_epoch = 20\n","save_folder = None\n","loss_filename = 'loss.h5'\n","\n","# optimizers\n","if optimizer_name == \"adagrad\":\n","  optimizer = optimizers.Adagrad(lr=lr)\n","  print(\"Used optimizer: adagrad, lr=\"+str(lr))\n","elif optimizer_name == \"adamax\":\n","  optimizer = optimizers.Adamax(lr=lr)\n","  print(\"Used optimizer: adamax, lr=\"+str(lr))\n","elif optimizer_name == \"nadam\":\n","  optimizer = optimizers.Nadam(lr=lr)  # recommended to leave at default params\n","  print(\"Used optimizer: nadam, lr=\"+str(lr))\n","elif optimizer_name == \"rms\":\n","  optimizer = optimizers.RMSprop(lr=lr)  # recommended for RNNs\n","  print(\"Used optimizer: rms, lr=\"+str(lr))\n","elif optimizer_name == \"SGD\":\n","  optimizer = optimizers.SGD(lr=lr)  # recommended for RNNs\n","  print(\"Used optimizer: SGD, lr=\"+str(lr))\n","elif optimizer_name == \"adadelta\":\n","  optimizer = optimizers.Adadelta(lr)  # recommended for RNNs\n","  print(\"Used optimizer: adadelta, lr=\"+str(lr))\n","else:\n","  optimizer = optimizers.Adam(lr=lr)\n","  print(\"Used optimizer: Adam, lr=\" + str(lr))\n","\n","# compile model\n","model.compile(optimizer, 'kullback_leibler_divergence', # categorial_crossentropy\n","                           metrics=['accuracy'])\n","if use_class_weights == True:\n","  hist = model.fit([X_train_LSTM, X_train_MLP],\n","                             y_train_one_hot,\n","                             validation_data=([X_test_LSTM, X_test_MLP], y_test_one_hot),\n","                             batch_size=batch_size, epochs=epochs, verbose=1, class_weight=class_weights,\n","                              callbacks=[\n","                               EarlyStoppingOnF1(epochs,\n","                                                 X_test_LSTM, X_test_MLP, y_test,\n","                                                 loss_filename, epsilon=0.0, min_epoch=min_epoch),])\n","else:\n","  hist = model.fit([X_train_LSTM, X_train_MLP],\n","                             y_train_one_hot,\n","                             validation_data=([X_test_LSTM, X_test_MLP], y_test_one_hot),\n","                             batch_size=batch_size, epochs=epochs, verbose=1,\n","                           callbacks=[\n","                               EarlyStoppingOnF1(epochs,\n","                                                 X_test_LSTM, X_test_MLP, y_test,\n","                                                 loss_filename, epsilon=0.0, min_epoch=min_epoch),])\n","  \n"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"gIzK9drzHIyM","executionInfo":{"status":"ok","timestamp":1665213294055,"user_tz":-210,"elapsed":25,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["model.save(\"save.h5\")"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"jETm9GZSL25i","executionInfo":{"status":"ok","timestamp":1665213321696,"user_tz":-210,"elapsed":27658,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["model.save('/dataset/models/model_v1.h5')"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"WV9kszEgJgH9","executionInfo":{"status":"ok","timestamp":1665213324143,"user_tz":-210,"elapsed":2463,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e82f7d3f-d977-432e-ab6d-517d3d45db4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model from:save.h5\n","X_train_head.shape = (400, 100)\n","X_train_doc.shape = (400, 20026)\n"]}],"source":["print(\"Loading model from:\" + \"save.h5\")\n","model = load_model(\"save.h5\")\n","if (model != None):\n","  X_test_LSTM, X_test_MLP = split_X(X_test, MAX_SEQ_LENGTH)\n","predicted_one_hot = model.predict([X_test_LSTM, X_test_MLP])\n","predicted_int = np.argmax(predicted_one_hot, axis=-1)"]},{"cell_type":"code","execution_count":81,"metadata":{"id":"-rvnwzcrJ_xc","executionInfo":{"status":"ok","timestamp":1665213324153,"user_tz":-210,"elapsed":71,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["y_test_str = le.inverse_transform(y_test)\n","y_pred_str = le.inverse_transform(predicted_int)"]},{"cell_type":"code","execution_count":82,"metadata":{"id":"W0Lz1i6vJF5-","executionInfo":{"status":"ok","timestamp":1665213324154,"user_tz":-210,"elapsed":71,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0f5abf3c-780a-4c1a-b03e-d0efe05aee60"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Discuss', 'Discuss', 'Discuss', 'Unrelated', 'Discuss', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Disagree', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Discuss', 'Unrelated', 'Discuss',\n","       'Discuss', 'Unrelated', 'Discuss', 'Discuss', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Discuss', 'Disagree',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Discuss', 'Unrelated',\n","       'Discuss', 'Unrelated', 'Unrelated', 'Discuss', 'Discuss',\n","       'Unrelated', 'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Discuss',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Discuss', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Disagree', 'Discuss',\n","       'Unrelated', 'Discuss', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Discuss', 'Disagree', 'Discuss',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Discuss', 'Discuss', 'Agree',\n","       'Disagree', 'Unrelated', 'Discuss', 'Discuss', 'Disagree', 'Agree',\n","       'Discuss', 'Discuss', 'Agree', 'Agree', 'Unrelated', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Agree', 'Unrelated', 'Discuss',\n","       'Unrelated', 'Discuss', 'Discuss', 'Discuss', 'Unrelated',\n","       'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Discuss', 'Unrelated', 'Disagree',\n","       'Discuss', 'Agree', 'Discuss', 'Discuss', 'Discuss', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Agree', 'Discuss', 'Discuss', 'Discuss',\n","       'Unrelated', 'Disagree', 'Disagree', 'Unrelated', 'Discuss',\n","       'Discuss', 'Discuss', 'Unrelated', 'Discuss', 'Discuss', 'Discuss',\n","       'Discuss', 'Unrelated', 'Discuss', 'Discuss', 'Discuss', 'Discuss',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Discuss', 'Unrelated', 'Agree', 'Discuss', 'Discuss', 'Discuss',\n","       'Discuss', 'Unrelated', 'Discuss', 'Discuss', 'Unrelated',\n","       'Discuss', 'Agree', 'Unrelated', 'Agree', 'Discuss', 'Unrelated',\n","       'Disagree', 'Discuss', 'Discuss', 'Disagree', 'Discuss', 'Discuss',\n","       'Discuss', 'Unrelated', 'Disagree', 'Unrelated', 'Unrelated',\n","       'Disagree', 'Agree', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Discuss', 'Discuss', 'Agree', 'Discuss', 'Discuss', 'Discuss',\n","       'Discuss', 'Discuss', 'Discuss', 'Unrelated', 'Disagree',\n","       'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Disagree', 'Discuss',\n","       'Discuss', 'Discuss', 'Discuss', 'Unrelated', 'Discuss', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Agree', 'Unrelated', 'Disagree',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Disagree', 'Discuss',\n","       'Disagree', 'Discuss', 'Discuss', 'Discuss', 'Disagree', 'Discuss',\n","       'Discuss', 'Discuss', 'Discuss', 'Disagree', 'Discuss', 'Discuss',\n","       'Disagree', 'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Disagree',\n","       'Discuss', 'Discuss', 'Discuss', 'Unrelated', 'Disagree',\n","       'Unrelated', 'Discuss', 'Discuss', 'Discuss', 'Unrelated', 'Agree',\n","       'Discuss', 'Agree', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Disagree', 'Disagree', 'Unrelated', 'Discuss', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Unrelated', 'Discuss',\n","       'Unrelated', 'Discuss', 'Discuss', 'Discuss', 'Agree', 'Discuss',\n","       'Discuss', 'Discuss', 'Agree', 'Disagree', 'Discuss', 'Discuss',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Discuss', 'Disagree',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Discuss', 'Unrelated',\n","       'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Unrelated',\n","       'Discuss', 'Discuss', 'Unrelated', 'Discuss', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Discuss', 'Discuss', 'Discuss',\n","       'Disagree', 'Discuss', 'Unrelated', 'Discuss', 'Discuss',\n","       'Discuss', 'Disagree', 'Discuss', 'Discuss', 'Unrelated',\n","       'Unrelated', 'Disagree', 'Unrelated', 'Discuss', 'Discuss',\n","       'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Unrelated',\n","       'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Discuss',\n","       'Discuss', 'Discuss', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Discuss', 'Discuss',\n","       'Unrelated', 'Discuss', 'Disagree', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Discuss', 'Discuss', 'Agree', 'Discuss', 'Disagree', 'Unrelated',\n","       'Discuss', 'Disagree', 'Agree', 'Discuss', 'Disagree', 'Discuss',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Agree', 'Disagree',\n","       'Disagree', 'Discuss', 'Discuss', 'Unrelated', 'Agree', 'Discuss',\n","       'Discuss', 'Unrelated', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Discuss', 'Unrelated', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Discuss', 'Agree',\n","       'Discuss', 'Disagree', 'Discuss', 'Unrelated', 'Discuss',\n","       'Discuss', 'Unrelated', 'Discuss', 'Discuss', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Unrelated', 'Unrelated'],\n","      dtype='<U9')"]},"metadata":{},"execution_count":82}],"source":["y_test_str"]},{"cell_type":"code","execution_count":83,"metadata":{"id":"EGhFM0zcJLdq","executionInfo":{"status":"ok","timestamp":1665213324155,"user_tz":-210,"elapsed":66,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b708f406-a604-40cd-c1ab-c4d07eeec28d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Unrelated', 'Unrelated', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Discuss', 'Discuss', 'Unrelated', 'Disagree', 'Discuss',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Disagree', 'Unrelated',\n","       'Unrelated', 'Disagree', 'Unrelated', 'Discuss', 'Discuss',\n","       'Discuss', 'Discuss', 'Disagree', 'Agree', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Disagree', 'Discuss',\n","       'Discuss', 'Unrelated', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Discuss', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Discuss', 'Unrelated',\n","       'Discuss', 'Discuss', 'Agree', 'Unrelated', 'Unrelated', 'Agree',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Unrelated', 'Agree',\n","       'Discuss', 'Unrelated', 'Unrelated', 'Agree', 'Discuss',\n","       'Unrelated', 'Discuss', 'Discuss', 'Discuss', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Agree', 'Discuss', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Agree', 'Unrelated', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Agree', 'Discuss', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Discuss', 'Agree',\n","       'Unrelated', 'Discuss', 'Discuss', 'Disagree', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Agree', 'Discuss', 'Discuss', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Discuss', 'Unrelated', 'Disagree', 'Unrelated', 'Unrelated',\n","       'Discuss', 'Unrelated', 'Discuss', 'Unrelated', 'Discuss',\n","       'Unrelated', 'Discuss', 'Agree', 'Unrelated', 'Discuss',\n","       'Disagree', 'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Agree', 'Unrelated', 'Discuss', 'Discuss', 'Agree', 'Unrelated',\n","       'Unrelated', 'Agree', 'Unrelated', 'Discuss', 'Discuss', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Discuss', 'Unrelated',\n","       'Agree', 'Unrelated', 'Discuss', 'Discuss', 'Unrelated', 'Discuss',\n","       'Discuss', 'Unrelated', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Unrelated', 'Discuss', 'Discuss', 'Agree', 'Discuss', 'Discuss',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Discuss', 'Disagree', 'Disagree', 'Unrelated', 'Unrelated',\n","       'Disagree', 'Disagree', 'Discuss', 'Agree', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Agree', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Discuss', 'Unrelated',\n","       'Discuss', 'Unrelated', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Discuss', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Discuss', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Disagree', 'Discuss',\n","       'Discuss', 'Unrelated', 'Agree', 'Disagree', 'Discuss', 'Disagree',\n","       'Discuss', 'Discuss', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Discuss', 'Discuss', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Discuss', 'Unrelated', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Discuss', 'Unrelated', 'Discuss', 'Agree', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Agree', 'Agree', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Disagree', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Agree', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Discuss', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Agree', 'Agree', 'Unrelated', 'Disagree',\n","       'Unrelated', 'Discuss', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Discuss', 'Agree', 'Discuss', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Discuss', 'Discuss', 'Unrelated', 'Unrelated',\n","       'Agree', 'Disagree', 'Unrelated', 'Agree', 'Unrelated', 'Disagree',\n","       'Unrelated', 'Discuss', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Unrelated', 'Disagree', 'Unrelated', 'Discuss', 'Discuss',\n","       'Discuss', 'Discuss', 'Unrelated', 'Discuss', 'Discuss',\n","       'Unrelated', 'Agree', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Discuss', 'Unrelated', 'Unrelated', 'Discuss', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Disagree', 'Unrelated',\n","       'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated', 'Unrelated',\n","       'Agree', 'Unrelated', 'Unrelated', 'Unrelated', 'Discuss',\n","       'Disagree', 'Unrelated', 'Discuss', 'Discuss', 'Agree',\n","       'Unrelated', 'Unrelated', 'Discuss', 'Discuss', 'Unrelated',\n","       'Unrelated'], dtype='<U9')"]},"metadata":{},"execution_count":83}],"source":["y_pred_str"]},{"cell_type":"code","execution_count":84,"metadata":{"id":"ChlIVSHMKeRe","executionInfo":{"status":"ok","timestamp":1665213324159,"user_tz":-210,"elapsed":60,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cf1b61a7-a45b-49ae-ecf4-8ed7dded7208"},"outputs":[{"output_type":"stream","name":"stdout","text":["FNC-1 score from restored model: 0.34010840108401086\n","\n"]}],"source":["score, cm = score_submission(y_test_str, y_pred_str)\n","fold_score, _ = score_submission(y_test_str, y_pred_str)\n","max_fold_score, _ = score_submission(y_test_str, y_test_str)\n","score = fold_score / max_fold_score\n","\n","print(\"FNC-1 score from restored model: \" +  str(score) +\"\\n\")"]},{"cell_type":"code","execution_count":85,"metadata":{"id":"Zlb4p0aCKtIy","executionInfo":{"status":"ok","timestamp":1665213324160,"user_tz":-210,"elapsed":50,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}}},"outputs":[],"source":["def print_confusion_matrix(cm):\n","    lines = []\n","    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n","    line_len = len(header)\n","    lines.append(\"-\"*line_len)\n","    lines.append(header)\n","    lines.append(\"-\"*line_len)\n","\n","    hit = 0\n","    total = 0\n","    for i, row in enumerate(cm):\n","        hit += row[i]\n","        total += sum(row)\n","        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n","                                                                   *row))\n","        lines.append(\"-\"*line_len)\n","    print('\\n'.join(lines))\n"]},{"cell_type":"code","execution_count":86,"metadata":{"id":"XYIs6CZmKyAc","executionInfo":{"status":"ok","timestamp":1665213324162,"user_tz":-210,"elapsed":52,"user":{"displayName":"mohammad test","userId":"01941297872756230090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d8435ef3-ba12-4fe4-82f5-b712414ed528"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------------------------\n","|           |   Agree   | Disagree  |  Discuss  | Unrelated |\n","-------------------------------------------------------------\n","|   Agree   |     3     |     0     |     5     |    14     |\n","-------------------------------------------------------------\n","| Disagree  |     2     |     4     |     6     |    25     |\n","-------------------------------------------------------------\n","|  Discuss  |    17     |    15     |    60     |    125    |\n","-------------------------------------------------------------\n","| Unrelated |     9     |     3     |    49     |    63     |\n","-------------------------------------------------------------\n"]}],"source":["print_confusion_matrix(cm)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}